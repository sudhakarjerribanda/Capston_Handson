{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install & pin packages (Colab)"
      ],
      "metadata": {
        "id": "Q7pKKQyeNBjm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqx5VCSJM6eK",
        "outputId": "5e8e4590-adad-462f-8be7-395d24accfbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pyarrow 15.0.2 requires numpy<2,>=1.16.6, but you have numpy 2.0.2 which is incompatible.\n",
            "gradio 5.49.0 requires fastapi<1.0,>=0.115.2, but you have fastapi 0.115.0 which is incompatible.\n",
            "gradio 5.49.0 requires starlette<1.0,>=0.40.0, but you have starlette 0.38.6 which is incompatible.\n",
            "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pyarrow 15.0.2 requires numpy<2,>=1.16.6, but you have numpy 2.3.3 which is incompatible.\n",
            "streamlit 1.38.0 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
            "streamlit 1.38.0 requires pillow<11,>=7.1.0, but you have pillow 11.3.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.3 which is incompatible.\n",
            "google-adk 1.14.1 requires starlette<1.0.0,>=0.46.2, but you have starlette 0.38.6 which is incompatible.\n",
            "google-adk 1.14.1 requires uvicorn<1.0.0,>=0.34.0, but you have uvicorn 0.30.6 which is incompatible.\n",
            "google-adk 1.14.1 requires watchdog<7.0.0,>=6.0.0, but you have watchdog 4.0.2 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.3 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.3 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.3 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.3 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.3 which is incompatible.\n",
            "gradio 5.49.0 requires fastapi<1.0,>=0.115.2, but you have fastapi 0.115.0 which is incompatible.\n",
            "gradio 5.49.0 requires starlette<1.0,>=0.40.0, but you have starlette 0.38.6 which is incompatible.\n",
            "mcp 1.16.0 requires pydantic<3.0.0,>=2.11.0, but you have pydantic 2.9.2 which is incompatible.\n",
            "mcp 1.16.0 requires uvicorn>=0.31.1; sys_platform != \"emscripten\", but you have uvicorn 0.30.6 which is incompatible.\n",
            "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.9.0 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.14.1 requires starlette<1.0.0,>=0.46.2, but you have starlette 0.38.6 which is incompatible.\n",
            "google-adk 1.14.1 requires uvicorn<1.0.0,>=0.34.0, but you have uvicorn 0.30.6 which is incompatible.\n",
            "google-adk 1.14.1 requires watchdog<7.0.0,>=6.0.0, but you have watchdog 4.0.2 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "gradio 5.49.0 requires fastapi<1.0,>=0.115.2, but you have fastapi 0.115.0 which is incompatible.\n",
            "gradio 5.49.0 requires starlette<1.0,>=0.40.0, but you have starlette 0.38.6 which is incompatible.\n",
            "mcp 1.16.0 requires pydantic<3.0.0,>=2.11.0, but you have pydantic 2.9.2 which is incompatible.\n",
            "mcp 1.16.0 requires uvicorn>=0.31.1; sys_platform != \"emscripten\", but you have uvicorn 0.30.6 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mPython: 3.12.12\n",
            "NumPy: 1.26.4\n",
            "transformers: 4.46.2\n",
            "diffusers: 0.31.0\n",
            "huggingface_hub: 0.33.5\n",
            "FastAPI: 0.115.0\n",
            "Streamlit: 1.38.0\n",
            "âœ… Environment ready\n"
          ]
        }
      ],
      "source": [
        "# A1) Minimal pins that don't fight Colab preinstalls\n",
        "%pip -q install --upgrade pip setuptools wheel\n",
        "\n",
        "# A2) Satisfy packages that complained earlier (numpy & jedi)\n",
        "%pip -q install \"numpy==2.0.2\" \"jedi>=0.19.1\"\n",
        "\n",
        "# A3) HF + Diffusers stack compatible with Colab & numpy 2.0.2\n",
        "%pip -q install --upgrade --force-reinstall \\\n",
        "  \"transformers==4.46.2\" \\\n",
        "  \"huggingface_hub==0.33.5\" \\\n",
        "  \"accelerate==0.31.0\" \\\n",
        "  \"diffusers==0.31.0\" \\\n",
        "  \"safetensors>=0.4.3\" \\\n",
        "  \"xformers>=0.0.27\" \\\n",
        "  \"pillow>=10.3.0\"\n",
        "\n",
        "# A4) Backend + Frontend\n",
        "%pip -q install \"fastapi==0.115.0\" \"uvicorn[standard]==0.30.6\" \"pyngrok==7.2.3\" \"nest_asyncio==1.6.0\" \"pydantic==2.9.2\" \"streamlit==1.38.0\" \"python-dotenv==1.0.1\"\n",
        "\n",
        "import sys, numpy, transformers, diffusers, huggingface_hub, fastapi, streamlit\n",
        "print(\"Python:\", sys.version.split()[0])\n",
        "print(\"NumPy:\", numpy.__version__)\n",
        "print(\"transformers:\", transformers.__version__)\n",
        "print(\"diffusers:\", diffusers.__version__)\n",
        "print(\"huggingface_hub:\", huggingface_hub.__version__)\n",
        "print(\"FastAPI:\", fastapi.__version__)\n",
        "print(\"Streamlit:\", streamlit.__version__)\n",
        "print(\"âœ… Environment ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B) Set secrets safely for this session (no hardcoding)"
      ],
      "metadata": {
        "id": "f-wT9gY7PgA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paste NEWLY ROTATED keys when prompted (input is hidden)\n",
        "import os, getpass\n",
        "\n",
        "print(\"Enter secrets (they will not be printed):\")\n",
        "GOOGLE_API_KEY = getpass.getpass(\"Google API key: \")\n",
        "OPENAI_API_KEY = getpass.getpass(\"OpenAI-style key: \")\n",
        "APP_API_TOKEN  = getpass.getpass(\"App token (for Streamlitâ†’FastAPI auth): \")\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "os.environ[\"APP_API_TOKEN\"]  = APP_API_TOKEN\n",
        "\n",
        "print(\"GOOGLE_API_KEY set:\", bool(os.environ.get(\"GOOGLE_API_KEY\")))\n",
        "print(\"OPENAI_API_KEY set:\", bool(os.environ.get(\"OPENAI_API_KEY\")))\n",
        "print(\"APP_API_TOKEN set:\",  bool(os.environ.get(\"APP_API_TOKEN\")))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kthUgsDENGH0",
        "outputId": "80a92ee8-05a1-4077-a31e-1ec400409013"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter secrets (they will not be printed):\n",
            "Google API key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "OpenAI-style key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "App token (for Streamlitâ†’FastAPI auth): Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "GOOGLE_API_KEY set: True\n",
            "OPENAI_API_KEY set: True\n",
            "APP_API_TOKEN set: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# C) Project layout + shared config"
      ],
      "metadata": {
        "id": "e7x1z0HtSTaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "ROOT = Path(\"Week_7\")\n",
        "(APP := ROOT/\"app\").mkdir(parents=True, exist_ok=True)\n",
        "(BACK := ROOT/\"backend\").mkdir(parents=True, exist_ok=True)\n",
        "(ROOT/\"diffusion\").mkdir(parents=True, exist_ok=True)\n",
        "(ROOT/\"screenshots\").mkdir(parents=True, exist_ok=True)\n",
        "(ROOT/\"lora_weights\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "run_cfg = {\n",
        "    \"model_id\": \"runwayml/stable-diffusion-v1-5\",\n",
        "    \"negative_prompt\": \"low quality, blurry, nsfw, text watermark, logo, distorted text\",\n",
        "    \"sd_steps\": 25,\n",
        "    \"sd_guidance\": 7.5,\n",
        "    \"size\": [512, 512],\n",
        "    \"guardrails\": {\"max_hops\": 3, \"unsafe_terms\": [\"nsfw\", \"violent\", \"illegal\"]},\n",
        "}\n",
        "(ROOT/\"week7_run_config.json\").write_text(json.dumps(run_cfg, indent=2))\n",
        "print(\"âœ… Wrote:\", (ROOT/\"week7_run_config.json\").resolve())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ManlsBJSU0s",
        "outputId": "6a766dc6-aa5a-47f6-ca77-d31895497df5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Wrote: /content/Week_7/week7_run_config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# D) Backend settings (reads env keys)"
      ],
      "metadata": {
        "id": "lRBo8mrvSZs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {BACK}/settings.py\n",
        "import os\n",
        "\n",
        "class Settings:\n",
        "    google_api_key: str = os.environ.get(\"GOOGLE_API_KEY\", \"\")\n",
        "    openai_api_key: str = os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
        "    app_api_token: str  = os.environ.get(\"APP_API_TOKEN\", \"\")  # bearer auth for UIâ†’API\n",
        "\n",
        "settings = Settings()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSadjD7aSa2X",
        "outputId": "129c8da4-307d-489d-f6db-dde5c1cd9bad"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Week_7/backend/settings.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# E) FastAPI backend: /qa, /generate, /agent, /health"
      ],
      "metadata": {
        "id": "koLxvltBSfyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {BACK}/main.py\n",
        "import time, os, uuid, json\n",
        "from pathlib import Path\n",
        "from typing import List, Optional, Dict\n",
        "from fastapi import FastAPI, HTTPException, Header\n",
        "from pydantic import BaseModel, Field\n",
        "import torch\n",
        "from PIL import Image\n",
        "from .settings import settings\n",
        "\n",
        "ROOT = Path(__file__).resolve().parents[1]\n",
        "OUT_DIR = ROOT / \"diffusion\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "CFG = json.loads((ROOT/\"week7_run_config.json\").read_text())\n",
        "\n",
        "MODEL_ID = CFG[\"model_id\"]\n",
        "NEGATIVE = CFG[\"negative_prompt\"]\n",
        "SD_STEPS = int(CFG[\"sd_steps\"])\n",
        "SD_GUIDE = float(CFG[\"sd_guidance\"])\n",
        "W, H = CFG[\"size\"]\n",
        "GUARD = CFG[\"guardrails\"]\n",
        "\n",
        "_sd_pipe = None\n",
        "def get_sd():\n",
        "    global _sd_pipe\n",
        "    if _sd_pipe is None:\n",
        "        from diffusers import StableDiffusionPipeline\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        _sd_pipe = StableDiffusionPipeline.from_pretrained(\n",
        "            MODEL_ID,\n",
        "            torch_dtype=torch.float16 if device==\"cuda\" else torch.float32\n",
        "        )\n",
        "        if device == \"cuda\":\n",
        "            _sd_pipe.enable_attention_slicing()\n",
        "            _sd_pipe.to(device)\n",
        "    return _sd_pipe\n",
        "\n",
        "def guardrails(text: str):\n",
        "    for bad in GUARD.get(\"unsafe_terms\", []):\n",
        "        if bad.lower() in text.lower():\n",
        "            raise HTTPException(status_code=400, detail=f\"Blocked by guardrails: {bad}\")\n",
        "\n",
        "def require_auth(authorization: str | None = Header(default=None)):\n",
        "    token = settings.app_api_token\n",
        "    if not token:  # auth disabled if not set\n",
        "        return\n",
        "    if not authorization or not authorization.startswith(\"Bearer \"):\n",
        "        raise HTTPException(status_code=401, detail=\"Missing bearer token\")\n",
        "    if authorization.split(\" \", 1)[1].strip() != token:\n",
        "        raise HTTPException(status_code=403, detail=\"Invalid token\")\n",
        "\n",
        "# ----- Schemas\n",
        "class QARequest(BaseModel):\n",
        "    query: str = Field(...)\n",
        "\n",
        "class QAResponse(BaseModel):\n",
        "    answer: str\n",
        "    citations: List[Dict[str,str]]\n",
        "    latency_ms: int\n",
        "\n",
        "class GenRequest(BaseModel):\n",
        "    prompt: str\n",
        "    negative_prompt: Optional[str] = NEGATIVE\n",
        "    steps: Optional[int] = SD_STEPS\n",
        "    guidance: Optional[float] = SD_GUIDE\n",
        "    height: Optional[int] = H\n",
        "    width: Optional[int] = W\n",
        "\n",
        "class GenResponse(BaseModel):\n",
        "    filename: str\n",
        "    latency_ms: int\n",
        "\n",
        "class AgentRequest(BaseModel):\n",
        "    input: str\n",
        "\n",
        "class AgentHop(BaseModel):\n",
        "    tool: str\n",
        "    input: str\n",
        "    output_preview: str\n",
        "    latency_ms: int\n",
        "\n",
        "class AgentResponse(BaseModel):\n",
        "    final: str\n",
        "    hops: List[AgentHop]\n",
        "    citations: List[Dict[str,str]] = []\n",
        "    image_filename: Optional[str] = None\n",
        "    latency_ms: int\n",
        "\n",
        "app = FastAPI(title=\"Week 7 Backend\", version=\"1.0\")\n",
        "\n",
        "@app.get(\"/health\")\n",
        "def health():\n",
        "    return {\"ok\": True, \"model\": MODEL_ID}\n",
        "\n",
        "# ----- Stub QA (replace with your Graph-RAG/Multi-Hop)\n",
        "def project_qa(query: str):\n",
        "    # If you call an external API, read keys from `settings.openai_api_key` / `settings.google_api_key`.\n",
        "    ans = f\"Stubbed answer for: {query} (replace with your Graph-RAG/Multi-Hop).\"\n",
        "    cites = [\n",
        "        {\"title\": \"Week 7 Repo\", \"url\": \"https://github.com/sudhakarjerribanda/Capston_Handson/tree/main/Week_7\"},\n",
        "        {\"title\": \"Task 1 Colab\", \"url\": \"https://colab.research.google.com/drive/1yZd5zUUmNjeDH-_mpVEkNCO4fonsmW69?usp=sharing\"}\n",
        "    ]\n",
        "    return ans, cites\n",
        "\n",
        "@app.post(\"/qa\", response_model=QAResponse)\n",
        "def qa(req: QARequest, _=require_auth()):\n",
        "    t0 = time.time()\n",
        "    guardrails(req.query)\n",
        "    ans, cites = project_qa(req.query)\n",
        "    return QAResponse(answer=ans, citations=cites, latency_ms=int((time.time()-t0)*1000))\n",
        "\n",
        "@app.post(\"/generate\", response_model=GenResponse)\n",
        "def generate(req: GenRequest, _=require_auth()):\n",
        "    t0 = time.time()\n",
        "    guardrails(req.prompt)\n",
        "    pipe = get_sd()\n",
        "    img = pipe(\n",
        "        prompt=req.prompt,\n",
        "        negative_prompt=req.negative_prompt or NEGATIVE,\n",
        "        num_inference_steps=req.steps or SD_STEPS,\n",
        "        guidance_scale=req.guidance or SD_GUIDE,\n",
        "        height=req.height or H,\n",
        "        width=req.width or W\n",
        "    ).images[0]\n",
        "    fname = f\"sd_{uuid.uuid4().hex[:8]}.png\"\n",
        "    img.save(OUT_DIR/fname)\n",
        "    return GenResponse(filename=fname, latency_ms=int((time.time()-t0)*1000))\n",
        "\n",
        "@app.post(\"/agent\", response_model=AgentResponse)\n",
        "def agent(req: AgentRequest, _=require_auth()):\n",
        "    t0 = time.time()\n",
        "    guardrails(req.input)\n",
        "    hops = []\n",
        "    keywords = [\"image\",\"diagram\",\"illustration\",\"infographic\",\"generate\",\"sd\"]\n",
        "    if any(k in req.input.lower() for k in keywords):\n",
        "        g0 = time.time()\n",
        "        prompt = req.input + \", flat vector, teal+indigo accents, clean typography, rounded cards, subtle shadows\"\n",
        "        pipe = get_sd()\n",
        "        img = pipe(\n",
        "            prompt=prompt, negative_prompt=NEGATIVE,\n",
        "            num_inference_steps=SD_STEPS, guidance_scale=SD_GUIDE,\n",
        "            height=H, width=W\n",
        "        ).images[0]\n",
        "        fname = f\"sd_{uuid.uuid4().hex[:8]}.png\"\n",
        "        img.save(OUT_DIR/fname)\n",
        "        hops.append(AgentHop(tool=\"stable_diffusion\", input=prompt, output_preview=fname, latency_ms=int((time.time()-g0)*1000)))\n",
        "        final = f\"Generated image {fname} for: {req.input}\"\n",
        "        cites = []\n",
        "        image = fname\n",
        "    else:\n",
        "        q0 = time.time()\n",
        "        ans, cites = project_qa(req.input)\n",
        "        hops.append(AgentHop(tool=\"project_qa\", input=req.input, output_preview=ans[:120], latency_ms=int((time.time()-q0)*1000)))\n",
        "        final = ans\n",
        "        image = None\n",
        "\n",
        "    # Append tiny metric\n",
        "    try:\n",
        "        mpath = ROOT/\"metrics.json\"\n",
        "        rec = {\"ts\": int(time.time()), \"input\": req.input, \"latency_ms\": int((time.time()-t0)*1000),\n",
        "               \"hops\": [h.model_dump() for h in hops], \"image\": image}\n",
        "        data = json.loads(mpath.read_text()) if mpath.exists() else []\n",
        "        data.append(rec); mpath.write_text(json.dumps(data, indent=2))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return AgentResponse(final=final, hops=hops, citations=cites, image_filename=image, latency_ms=int((time.time()-t0)*1000))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6IqEDHLScWe",
        "outputId": "44dbf5e1-79a1-4330-9cc0-ac91e1d6c804"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Week_7/backend/main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# F) Start backend + expose with ngrok"
      ],
      "metadata": {
        "id": "aH8HklWpSrHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… Start FastAPI (port 8000) and expose: ngrok if token, else cloudflared\n",
        "import os, time, threading, re, subprocess, requests\n",
        "from pathlib import Path\n",
        "import nest_asyncio, uvicorn\n",
        "\n",
        "# --- 1) Run the FastAPI backend on 8000 ---\n",
        "nest_asyncio.apply()\n",
        "\n",
        "def run_api():\n",
        "    uvicorn.run(\"Week_7.backend.main:app\", host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
        "\n",
        "thread = threading.Thread(target=run_api, daemon=True)\n",
        "thread.start()\n",
        "time.sleep(2)  # give server a moment\n",
        "\n",
        "# --- 2) Helpers to open a public URL ---\n",
        "def _ensure_cloudflared():\n",
        "    if not Path(\"cloudflared\").exists():\n",
        "        # Download static binary\n",
        "        !curl -L https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -o cloudflared -#\n",
        "        !chmod +x cloudflared\n",
        "\n",
        "def _start_cloudflared(port: int):\n",
        "    \"\"\"Start a Cloudflare tunnel and return (public_url, process).\"\"\"\n",
        "    _ensure_cloudflared()\n",
        "    proc = subprocess.Popen(\n",
        "        [\"./cloudflared\", \"tunnel\", \"--url\", f\"http://127.0.0.1:{port}\", \"--no-autoupdate\"],\n",
        "        stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1\n",
        "    )\n",
        "    public_url = None\n",
        "    start = time.time()\n",
        "    while time.time() - start < 30:\n",
        "        line = proc.stdout.readline()\n",
        "        if not line:\n",
        "            time.sleep(0.1); continue\n",
        "        m = re.search(r\"https://[-\\w]+\\.trycloudflare\\.com\", line.strip())\n",
        "        if m:\n",
        "            public_url = m.group(0)\n",
        "            break\n",
        "    if not public_url:\n",
        "        proc.terminate()\n",
        "        raise RuntimeError(\"cloudflared did not produce a public URL in time.\")\n",
        "    return public_url, proc\n",
        "\n",
        "def _start_ngrok(port: int):\n",
        "    \"\"\"Start ngrok if NGROK_AUTHTOKEN is set. Return (public_url, None).\"\"\"\n",
        "    from pyngrok import ngrok, conf\n",
        "    token = os.environ.get(\"NGROK_AUTHTOKEN\", \"\").strip()\n",
        "    if not token:\n",
        "        raise RuntimeError(\"NGROK_AUTHTOKEN not set\")\n",
        "    conf.get_default().auth_token = token\n",
        "    # Close any existing tunnels cleanly\n",
        "    for t in ngrok.get_tunnels():\n",
        "        try: ngrok.disconnect(t.public_url)\n",
        "        except: pass\n",
        "    url = ngrok.connect(addr=port, proto=\"http\").public_url\n",
        "    return url, None\n",
        "\n",
        "def expose_port(port: int):\n",
        "    \"\"\"Try ngrok if token present; otherwise fallback to cloudflared.\"\"\"\n",
        "    if os.environ.get(\"NGROK_AUTHTOKEN\", \"\").strip():\n",
        "        url, proc = _start_ngrok(port)\n",
        "        print(f\"âœ… ngrok tunnel ready on port {port}: {url}\")\n",
        "        return url, proc\n",
        "    else:\n",
        "        url, proc = _start_cloudflared(port)\n",
        "        print(f\"âœ… cloudflared tunnel ready on port {port}: {url}\")\n",
        "        return url, proc\n",
        "\n",
        "# --- 3) Expose backend and save URL for Streamlit ---\n",
        "backend_url, backend_proc = expose_port(8000)\n",
        "Path(\"BACKEND_URL.txt\").write_text(backend_url)\n",
        "print(\"Backend URL:\", backend_url)\n",
        "print(\"Health     :\", f\"{backend_url}/health\")\n",
        "\n",
        "# --- 4) Quick health check ---\n",
        "try:\n",
        "    print(\"Health JSON:\", requests.get(f\"{backend_url}/health\", timeout=20).json())\n",
        "except Exception as e:\n",
        "    print(\"Health check failed:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dhpi0jXjSje_",
        "outputId": "0bd1be47-f631-4570-fae6-932533966efe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-4 (run_api):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1012, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/tmp/ipython-input-2816236014.py\", line 10, in run_api\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/main.py\", line 577, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 65, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\", line 26, in run\n",
            "    loop = asyncio.get_event_loop()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\", line 40, in _get_event_loop\n",
            "    loop = events.get_event_loop_policy().get_event_loop()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\", line 67, in get_event_loop\n",
            "    _patch_loop(loop)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\", line 193, in _patch_loop\n",
            "    raise ValueError('Can\\'t patch loop of type %s' % type(loop))\n",
            "ValueError: Can't patch loop of type <class 'uvloop.Loop'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… cloudflared tunnel ready on port 8000: https://missouri-complicated-chelsea-boat.trycloudflare.com\n",
            "Backend URL: https://missouri-complicated-chelsea-boat.trycloudflare.com\n",
            "Health     : https://missouri-complicated-chelsea-boat.trycloudflare.com/health\n",
            "Health check failed: HTTPSConnectionPool(host='missouri-complicated-chelsea-boat.trycloudflare.com', port=443): Max retries exceeded with url: /health (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7ccb4d681610>: Failed to resolve 'missouri-complicated-chelsea-boat.trycloudflare.com' ([Errno -2] Name or service not known)\"))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## G) Smoke tests (with bearer auth if set)"
      ],
      "metadata": {
        "id": "-pSu_OcEWA6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, requests, json, textwrap\n",
        "from pathlib import Path\n",
        "\n",
        "LOCAL = \"http://127.0.0.1:8000/health\"\n",
        "PUBLIC = (Path(\"BACKEND_URL.txt\").read_text().strip().rstrip(\"/\")) + \"/health\"\n",
        "\n",
        "def peek(url):\n",
        "    try:\n",
        "        r = requests.get(url, timeout=20)\n",
        "        ct = r.headers.get(\"content-type\", \"\")\n",
        "        print(f\"\\nURL: {url}\")\n",
        "        print(f\"Status: {r.status_code} | Content-Type: {ct}\")\n",
        "        body = r.text\n",
        "        print(\"Body preview:\")\n",
        "        print(textwrap.shorten(body.replace(\"\\n\",\" \")[:400], width=400, placeholder=\" â€¦\"))\n",
        "        return r\n",
        "    except Exception as e:\n",
        "        print(f\"\\nURL: {url} -> ERROR: {e}\")\n",
        "        return None\n",
        "\n",
        "r_local = peek(LOCAL)      # should be JSON from FastAPI if backend is up\n",
        "r_public = peek(PUBLIC)    # if this is HTML, itâ€™s the tunnel page or an error page\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Tbims1GWUKT",
        "outputId": "cf5c88d6-ed2f-4574-deeb-08f8796d4cb9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "URL: http://127.0.0.1:8000/health -> ERROR: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /health (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ccb4d9104d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "\n",
            "URL: https://missouri-complicated-chelsea-boat.trycloudflare.com/health\n",
            "Status: 502 | Content-Type: text/plain; charset=utf-8\n",
            "Body preview:\n",
            "502 Bad Gateway Unable to reach the origin service. The service may be down or it may not be responding to traffic from cloudflared\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests, json\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"BACKEND_URL.txt\").read_text().strip().rstrip(\"/\")\n",
        "headers = {\"Authorization\": f\"Bearer {os.environ['APP_API_TOKEN']}\"} if os.environ.get(\"APP_API_TOKEN\") else {}\n",
        "\n",
        "def get_json(url, method=\"GET\", **kwargs):\n",
        "    try:\n",
        "        r = requests.request(method, url, timeout=kwargs.pop(\"timeout\", 30), **kwargs)\n",
        "        ct = r.headers.get(\"content-type\",\"\")\n",
        "        print(f\"\\n{method} {url} -> {r.status_code} {ct}\")\n",
        "        if \"application/json\" in ct.lower():\n",
        "            data = r.json()\n",
        "            print(json.dumps(data, indent=2)[:400], \"...\")\n",
        "            return data\n",
        "        else:\n",
        "            # Not JSON â€“ show a preview, don't raise\n",
        "            print(r.text[:400], \"...\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Request error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Health\n",
        "get_json(f\"{BASE}/health\", \"GET\")\n",
        "\n",
        "# QA\n",
        "get_json(f\"{BASE}/qa\", \"POST\", json={\"query\":\"Explain the IMRaD template in our medical text app\"}, headers=headers, timeout=60)\n",
        "\n",
        "# Generate\n",
        "get_json(f\"{BASE}/generate\", \"POST\", json={\"prompt\":\"infographic of a trust game with BDI panels, teal+indigo, flat vector, presentation-ready\"}, headers=headers, timeout=180)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkGhQLhnWAg4",
        "outputId": "1e410e85-2254-42dd-ecfa-1888fb7c5ed8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GET https://missouri-complicated-chelsea-boat.trycloudflare.com/health -> 502 text/plain; charset=utf-8\n",
            "502 Bad Gateway\n",
            "Unable to reach the origin service. The service may be down or it may not be responding to traffic from cloudflared\n",
            " ...\n",
            "\n",
            "POST https://missouri-complicated-chelsea-boat.trycloudflare.com/qa -> 502 text/plain; charset=utf-8\n",
            "502 Bad Gateway\n",
            "Unable to reach the origin service. The service may be down or it may not be responding to traffic from cloudflared\n",
            " ...\n",
            "\n",
            "POST https://missouri-complicated-chelsea-boat.trycloudflare.com/generate -> 502 text/plain; charset=utf-8\n",
            "502 Bad Gateway\n",
            "Unable to reach the origin service. The service may be down or it may not be responding to traffic from cloudflared\n",
            " ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# H) Streamlit UI (writes Week_7/app/app.py)"
      ],
      "metadata": {
        "id": "yzgH55FGW1n8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Week_7/app/app.py\n",
        "import os, time, json, requests\n",
        "from pathlib import Path\n",
        "import streamlit as st\n",
        "\n",
        "st.set_page_config(page_title=\"Week 7 App\", page_icon=\"ðŸ§ª\", layout=\"wide\")\n",
        "\n",
        "# Resolve backend URL: secrets -> env -> file -> default local\n",
        "BACKEND_URL    = st.secrets.get(\"BACKEND_URL\", os.environ.get(\"BACKEND_URL\", \"\"))\n",
        "APP_API_TOKEN  = st.secrets.get(\"APP_API_TOKEN\", os.environ.get(\"APP_API_TOKEN\", \"\"))\n",
        "\n",
        "if not BACKEND_URL and Path(\"BACKEND_URL.txt\").exists():\n",
        "    BACKEND_URL = Path(\"BACKEND_URL.txt\").read_text().strip()\n",
        "\n",
        "if not BACKEND_URL:\n",
        "    BACKEND_URL = \"http://127.0.0.1:8000\"  # fallback\n",
        "\n",
        "headers = {\"Authorization\": f\"Bearer {APP_API_TOKEN}\"} if APP_API_TOKEN else {}\n",
        "\n",
        "st.sidebar.success(\"Backend: \" + BACKEND_URL)\n",
        "st.title(\"CS 5588 â€“ Week 7 App (Sudhakar Reddy Jerribanda)\")\n",
        "tabs = st.tabs([\"Ask (QA)\", \"Generate (SD)\", \"Agent\"])\n",
        "\n",
        "with tabs[0]:\n",
        "    st.subheader(\"Ask your project model\")\n",
        "    q = st.text_input(\"Question\", \"How do we visualize trust (BDI) in the app?\")\n",
        "    if st.button(\"Run QA\"):\n",
        "        try:\n",
        "            r = requests.post(f\"{BACKEND_URL.rstrip('/')}/qa\", json={\"query\": q}, headers=headers, timeout=120)\n",
        "            ct = (r.headers.get(\"content-type\") or \"\").lower()\n",
        "            if \"application/json\" in ct:\n",
        "                data = r.json()\n",
        "                st.markdown(\"**Answer:**\")\n",
        "                st.write(data.get(\"answer\",\"\"))\n",
        "                st.markdown(\"**Citations:**\")\n",
        "                for c in data.get(\"citations\", []):\n",
        "                    st.write(f\"- [{c.get('title','link')}]({c.get('url','#')})\")\n",
        "                st.caption(f\"Latency: {data.get('latency_ms',0)} ms\")\n",
        "            else:\n",
        "                st.warning(\"Non-JSON response from backend. Preview below:\")\n",
        "                st.code(r.text[:600])\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error: {e}\")\n",
        "\n",
        "with tabs[1]:\n",
        "    st.subheader(\"Generate a project-style visual (Stable Diffusion)\")\n",
        "    prompt = st.text_area(\"Prompt\", \"infographic of a trust game with BDI panels, teal+indigo, flat vector, presentation-ready\", height=100)\n",
        "    if st.button(\"Generate Image\"):\n",
        "        try:\n",
        "            r = requests.post(f\"{BACKEND_URL.rstrip('/')}/generate\", json={\"prompt\": prompt}, headers=headers, timeout=300)\n",
        "            ct = (r.headers.get(\"content-type\") or \"\").lower()\n",
        "            if \"application/json\" in ct:\n",
        "                data = r.json()\n",
        "                fname = data.get(\"filename\")\n",
        "                if fname:\n",
        "                    st.image(str(Path(\"Week_7\")/\"diffusion\"/fname), caption=fname, use_column_width=True)\n",
        "                st.caption(f\"Latency: {data.get('latency_ms',0)} ms\")\n",
        "            else:\n",
        "                st.warning(\"Non-JSON response from backend. Preview below:\")\n",
        "                st.code(r.text[:600])\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error: {e}\")\n",
        "\n",
        "with tabs[2]:\n",
        "    st.subheader(\"Agent Orchestration (Plan â†’ Execute â†’ Aggregate)\")\n",
        "    ipt = st.text_area(\"Ask or request a visual\", \"Generate a clean isometric diagram of recruiter â†’ MDT â†’ ICT triage flow\")\n",
        "    if st.button(\"Run Agent\"):\n",
        "        try:\n",
        "            r = requests.post(f\"{BACKEND_URL.rstrip('/')}/agent\", json={\"input\": ipt}, headers=headers, timeout=300)\n",
        "            ct = (r.headers.get(\"content-type\") or \"\").lower()\n",
        "            if \"application/json\" in ct:\n",
        "                data = r.json()\n",
        "                st.markdown(\"**Final:**\")\n",
        "                st.write(data.get(\"final\",\"\"))\n",
        "                st.markdown(\"**Hops (trace):**\")\n",
        "                for h in data.get(\"hops\", []):\n",
        "                    st.code(json.dumps(h, indent=2))\n",
        "                img = data.get(\"image_filename\")\n",
        "                if img:\n",
        "                    st.image(str(Path(\"Week_7\")/\"diffusion\"/img), caption=img, use_column_width=True)\n",
        "                if data.get(\"citations\"):\n",
        "                    st.markdown(\"**Citations:**\")\n",
        "                    for c in data[\"citations\"]:\n",
        "                        st.write(f\"- [{c.get('title','link')}]({c.get('url','#')})\")\n",
        "                st.caption(f\"Latency: {data.get('latency_ms',0)} ms\")\n",
        "            else:\n",
        "                st.warning(\"Non-JSON response from backend. Preview below:\")\n",
        "                st.code(r.text[:600])\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeLhnlW6WEnf",
        "outputId": "0e814ac2-1fab-461f-867e-cff408c2422b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Week_7/app/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I) Launch Streamlit and expose (reuses expose_port; defines a fallback if missing)"
      ],
      "metadata": {
        "id": "QHp6x-z6W6h5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- I) Launch Streamlit and expose on 8501 ----\n",
        "import os, threading, time, subprocess, re\n",
        "from pathlib import Path\n",
        "\n",
        "# If expose_port isn't defined (e.g., new session), define a minimal fallback (cloudflared only)\n",
        "if \"expose_port\" not in globals():\n",
        "    import subprocess, time, re\n",
        "    def _ensure_cloudflared():\n",
        "        if not Path(\"cloudflared\").exists():\n",
        "            !curl -L https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -o cloudflared -#\n",
        "            !chmod +x cloudflared\n",
        "    def expose_port(port: int):\n",
        "        _ensure_cloudflared()\n",
        "        proc = subprocess.Popen(\n",
        "            [\"./cloudflared\", \"tunnel\", \"--url\", f\"http://127.0.0.1:{port}\", \"--no-autoupdate\"],\n",
        "            stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1\n",
        "        )\n",
        "        public_url = None; start = time.time()\n",
        "        while time.time() - start < 30:\n",
        "            line = proc.stdout.readline()\n",
        "            if not line: time.sleep(0.1); continue\n",
        "            m = re.search(r\"https://[-\\w]+\\.trycloudflare\\.com\", line.strip())\n",
        "            if m: public_url = m.group(0); break\n",
        "        if not public_url:\n",
        "            proc.terminate(); raise RuntimeError(\"cloudflared did not produce a public URL in time.\")\n",
        "        return public_url, proc\n",
        "\n",
        "# Pass backend URL/token to the Streamlit process via env (so app.py can read them if needed)\n",
        "BASE = (Path(\"BACKEND_URL.txt\").read_text().strip() if Path(\"BACKEND_URL.txt\").exists() else \"http://127.0.0.1:8000\")\n",
        "os.environ[\"BACKEND_URL\"]   = BASE\n",
        "os.environ[\"APP_API_TOKEN\"] = os.environ.get(\"APP_API_TOKEN\",\"\")\n",
        "\n",
        "def run_streamlit():\n",
        "    subprocess.Popen([\n",
        "        \"streamlit\", \"run\", str(Path(\"Week_7\")/\"app\"/\"app.py\"),\n",
        "        \"--server.address=0.0.0.0\", \"--server.port=8501\"\n",
        "    ])\n",
        "\n",
        "threading.Thread(target=run_streamlit, daemon=True).start()\n",
        "time.sleep(3)  # give Streamlit time to boot\n",
        "\n",
        "frontend_url, frontend_proc = expose_port(8501)\n",
        "print(\"âœ… Backend:\", BASE)\n",
        "print(\"âœ… Frontend:\", frontend_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NSEiOBVW3qk",
        "outputId": "dad608a3-c3ef-4872-ae5b-e9d01a7093a9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… cloudflared tunnel ready on port 8501: https://members-watts-soft-afternoon.trycloudflare.com\n",
            "âœ… Backend: https://missouri-complicated-chelsea-boat.trycloudflare.com\n",
            "âœ… Frontend: https://members-watts-soft-afternoon.trycloudflare.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Week_7/app/app.py\n",
        "import os, json, requests\n",
        "from pathlib import Path\n",
        "import streamlit as st\n",
        "\n",
        "st.set_page_config(page_title=\"Week 7 App\", page_icon=\"ðŸ§ª\", layout=\"wide\")\n",
        "\n",
        "def safe_secret(key: str, default: str = \"\") -> str:\n",
        "    # Prefer environment (works in Colab), then secrets if available.\n",
        "    val = os.environ.get(key)\n",
        "    if val:\n",
        "        return val\n",
        "    try:\n",
        "        # Only try st.secrets if a secrets file exists\n",
        "        # (Streamlit raises FileNotFoundError when no secrets.toml)\n",
        "        _ = st.secrets  # access to trigger load; will raise if missing\n",
        "        return st.secrets.get(key, default)\n",
        "    except Exception:\n",
        "        return default\n",
        "\n",
        "# Resolve backend URL/token with robust fallback order:\n",
        "# 1) ENV  2) secrets.toml (if present)  3) BACKEND_URL.txt  4) local default\n",
        "BACKEND_URL   = safe_secret(\"BACKEND_URL\", \"\")\n",
        "APP_API_TOKEN = safe_secret(\"APP_API_TOKEN\", \"\")\n",
        "\n",
        "if not BACKEND_URL and Path(\"BACKEND_URL.txt\").exists():\n",
        "    BACKEND_URL = Path(\"BACKEND_URL.txt\").read_text().strip()\n",
        "\n",
        "if not BACKEND_URL:\n",
        "    BACKEND_URL = \"http://127.0.0.1:8000\"\n",
        "\n",
        "headers = {\"Authorization\": f\"Bearer {APP_API_TOKEN}\"} if APP_API_TOKEN else {}\n",
        "\n",
        "st.sidebar.success(\"Backend: \" + BACKEND_URL)\n",
        "st.title(\"CS 5588 â€“ Week 7 App (Sudhakar Reddy Jerribanda)\")\n",
        "tabs = st.tabs([\"Ask (QA)\", \"Generate (SD)\", \"Agent\"])\n",
        "\n",
        "with tabs[0]:\n",
        "    st.subheader(\"Ask your project model\")\n",
        "    q = st.text_input(\"Question\", \"How do we visualize trust (BDI) in the app?\")\n",
        "    if st.button(\"Run QA\"):\n",
        "        try:\n",
        "            r = requests.post(f\"{BACKEND_URL.rstrip('/')}/qa\", json={\"query\": q}, headers=headers, timeout=120)\n",
        "            ct = (r.headers.get(\"content-type\") or \"\").lower()\n",
        "            if \"application/json\" in ct:\n",
        "                data = r.json()\n",
        "                st.markdown(\"**Answer:**\")\n",
        "                st.write(data.get(\"answer\",\"\"))\n",
        "                st.markdown(\"**Citations:**\")\n",
        "                for c in data.get(\"citations\", []):\n",
        "                    st.write(f\"- [{c.get('title','link')}]({c.get('url','#')})\")\n",
        "                st.caption(f\"Latency: {data.get('latency_ms',0)} ms\")\n",
        "            else:\n",
        "                st.warning(\"Non-JSON response from backend. Preview below:\")\n",
        "                st.code(r.text[:600])\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error: {e}\")\n",
        "\n",
        "with tabs[1]:\n",
        "    st.subheader(\"Generate a project-style visual (Stable Diffusion)\")\n",
        "    prompt = st.text_area(\"Prompt\", \"infographic of a trust game with BDI panels, teal+indigo, flat vector, presentation-ready\", height=100)\n",
        "    if st.button(\"Generate Image\"):\n",
        "        try:\n",
        "            r = requests.post(f\"{BACKEND_URL.rstrip('/')}/generate\", json={\"prompt\": prompt}, headers=headers, timeout=300)\n",
        "            ct = (r.headers.get(\"content-type\") or \"\").lower()\n",
        "            if \"application/json\" in ct:\n",
        "                data = r.json()\n",
        "                fname = data.get(\"filename\")\n",
        "                if fname:\n",
        "                    st.image(str(Path(\"Week_7\")/\"diffusion\"/fname), caption=fname, use_column_width=True)\n",
        "                st.caption(f\"Latency: {data.get('latency_ms',0)} ms\")\n",
        "            else:\n",
        "                st.warning(\"Non-JSON response from backend. Preview below:\")\n",
        "                st.code(r.text[:600])\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error: {e}\")\n",
        "\n",
        "with tabs[2]:\n",
        "    st.subheader(\"Agent Orchestration (Plan â†’ Execute â†’ Aggregate)\")\n",
        "    ipt = st.text_area(\"Ask or request a visual\", \"Generate a clean isometric diagram of recruiter â†’ MDT â†’ ICT triage flow\")\n",
        "    if st.button(\"Run Agent\"):\n",
        "        try:\n",
        "            r = requests.post(f\"{BACKEND_URL.rstrip('/')}/agent\", json={\"input\": ipt}, headers=headers, timeout=300)\n",
        "            ct = (r.headers.get(\"content-type\") or \"\").lower()\n",
        "            if \"application/json\" in ct:\n",
        "                data = r.json()\n",
        "                st.markdown(\"**Final:**\")\n",
        "                st.write(data.get(\"final\",\"\"))\n",
        "                st.markdown(\"**Hops (trace):**\")\n",
        "                for h in data.get(\"hops\", []):\n",
        "                    st.code(json.dumps(h, indent=2))\n",
        "                img = data.get(\"image_filename\")\n",
        "                if img:\n",
        "                    st.image(str(Path(\"Week_7\")/\"diffusion\"/img), caption=img, use_column_width=True)\n",
        "                if data.get(\"citations\"):\n",
        "                    st.markdown(\"**Citations:**\")\n",
        "                    for c in data[\"citations\"]:\n",
        "                        st.write(f\"- [{c.get('title','link')}]({c.get('url','#')})\")\n",
        "                st.caption(f\"Latency: {data.get('latency_ms',0)} ms\")\n",
        "            else:\n",
        "                st.warning(\"Non-JSON response from backend. Preview below:\")\n",
        "                st.code(r.text[:600])\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFp-cyvkW-JJ",
        "outputId": "2803c177-2982-48d1-d81f-15aba4433fc8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Week_7/app/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: create a secrets.toml so Streamlit can read from it\n",
        "from pathlib import Path\n",
        "Path(\"/content/.streamlit\").mkdir(parents=True, exist_ok=True)\n",
        "Path(\"/content/.streamlit/secrets.toml\").write_text(\n",
        "    'BACKEND_URL = \"{}\"\\nAPP_API_TOKEN = \"{}\"\\n'.format(\n",
        "        os.environ.get(\"BACKEND_URL\",\"http://127.0.0.1:8000\"),\n",
        "        os.environ.get(\"APP_API_TOKEN\",\"\")\n",
        "    )\n",
        ")\n",
        "print(\"Wrote /content/.streamlit/secrets.toml\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOHb7sPaXZX6",
        "outputId": "43de0507-c6f8-4908-dd80-1c4a0c6a189f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote /content/.streamlit/secrets.toml\n"
          ]
        }
      ]
    }
  ]
}