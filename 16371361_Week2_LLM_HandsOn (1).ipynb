{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b975fd2",
   "metadata": {
    "id": "9b975fd2"
   },
   "source": [
    "# ðŸš€ 45-Minute Hands-On: LLMs with Hugging Face (Colab/Jupyter)\n",
    "\n",
    "**Last updated:** 2025-09-01 05:29\n",
    "\n",
    "## Goals\n",
    "- Run a small **instruction-tuned LLM** with ðŸ¤— Transformers\n",
    "- Use the **pipeline** API\n",
    "- Tune decoding (temperature, top-p, top-k)\n",
    "- Build a tiny **chat loop**\n",
    "- Batch prompts â†’ CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a672790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Install dependencies\n",
    "!pip -q install -U transformers accelerate datasets sentencepiece pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c965853c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 2) Imports & device\n",
    "import torch, time\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521f3f72",
   "metadata": {
    "id": "521f3f72"
   },
   "source": [
    "## Model choice\n",
    "We try **TinyLlama/TinyLlama-1.1B-Chat-v1.0** and fall back to **distilgpt2** if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4defa5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary failed: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/google/gemma-2b-it.\n",
      "401 Client Error. (Request ID: Root=1-68ba651a-0f32c3b47509822930064732;99c73160-33ab-4b98-bbdb-a5b2d3b1b9b0)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/google/gemma-2b-it/resolve/main/config.json.\n",
      "Access to model google/gemma-2b-it is restricted. You must have access to it and be authenticated to access it. Please log in. \n",
      "Falling back to microsoft/phi-2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c6188a6f0f401fb04e8a8dab97e328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e5f7b769654fafae0405217653ab16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea9671237a4d4bd1bcfc2817ff74bb71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcfe7ef739cd47f783e77e347c3f7415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770ae0ca9dd346d1bd0ce747dda3467f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e9a4019a2d43c89650f016abdf5121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b36dd26ec2b4017a9b2270c8b6cd2bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f8e4be776a46c5ba8ab6d2618b6fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f60e3ecce3a43db9c1c7c92f883ac9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1beaab52cc4f4dc5a26fc24bb0ec38bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba30526b7934a559365183ee62e98c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49cfcb77e6df4876aad3ea8438f6f6ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ea99421db94f29ae803693898a2e76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: microsoft/phi-2\n",
      "\n",
      "Model Response:\n",
      " You are part of a multi-agent LLM system designed for healthcare.\n",
      "Agents have different roles:\n",
      "- Summarizer Agent: Condenses long medical records into short notes.\n",
      "- Research Agent: Drafts structured medical research articles.\n",
      "- Privacy Agent: Removes PHI from sensitive data.\n",
      "\n",
      "Question: \n",
      "How can these agents collaborate to support doctors in clinical decision-making\n",
      "while ensuring patient data privacy and compliance with HIPAA?\n",
      "\n",
      "\n",
      "\n",
      "The first step is to understand the roles of each agent. The Summarizer Agent condenses long medical records into short notes, the Research Agent drafts structured medical research articles, and the Privacy Agent removes PHI from sensitive data.\n",
      "\n",
      "The next step is to identify the common goal of all agents, which is to support doctors in clinical decision-making. This can be achieved by integrating the work of all agents.\n",
      "\n",
      "The Summarizer Agent can provide the Research Agent with condensed medical records, which can be used as a basis for drafting structured medical research articles.\n",
      "\n",
      "The Research Agent can then draft structured medical research articles based on the condensed medical records provided by the Summarizer Agent.\n",
      "\n",
      "The Privacy Agent can remove PHI from the sensitive data used by the Research Agent, ensuring patient data privacy.\n",
      "\n",
      "The final step is to ensure that the collaboration between the agents complies with HIPAA regulations. This can be achieved by implementing secure communication channels and data encryption.\n",
      "\n",
      "Answer: \n",
      "The agents can collaborate by integrating their work. The Summarizer Agent provides condensed medical records to the Research Agent, who drafts structured medical research articles. The Privacy Agent removes PHI from the sensitive data used by the Research Agent\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Updated models\n",
    "model_id = \"google/gemma-2b-it\"        # Primary (instruction-tuned)\n",
    "fallback_model_id = \"microsoft/phi-2\" # Strong lightweight fallback\n",
    "\n",
    "def load_model(model_name):\n",
    "    try:\n",
    "        tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        mdl = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "        )\n",
    "        return tok, mdl, model_name\n",
    "    except Exception as e:\n",
    "        print(\"Primary failed:\", e, \"\\nFalling back to\", fallback_model_id)\n",
    "        tok = AutoTokenizer.from_pretrained(fallback_model_id, use_fast=True)\n",
    "        mdl = AutoModelForCausalLM.from_pretrained(\n",
    "            fallback_model_id,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "        )\n",
    "        return tok, mdl, fallback_model_id\n",
    "\n",
    "# Load model\n",
    "tokenizer, model, active_model_id = load_model(model_id)\n",
    "print(\"Loaded:\", active_model_id)\n",
    "\n",
    "# Example healthcare multi-agent LLM prompt\n",
    "prompt = \"\"\"You are part of a multi-agent LLM system designed for healthcare.\n",
    "Agents have different roles:\n",
    "- Summarizer Agent: Condenses long medical records into short notes.\n",
    "- Research Agent: Drafts structured medical research articles.\n",
    "- Privacy Agent: Removes PHI from sensitive data.\n",
    "\n",
    "Question:\n",
    "How can these agents collaborate to support doctors in clinical decision-making\n",
    "while ensuring patient data privacy and compliance with HIPAA?\"\"\"\n",
    "\n",
    "# Generate response\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=250)\n",
    "print(\"\\nModel Response:\\n\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7MA8phCCiDd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary failed: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/google/gemma-2b-it.\n",
      "401 Client Error. (Request ID: Root=1-68ba6642-560531ca031d3cf544005473;9ee53045-b443-4f03-adf2-1512f73230d2)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/google/gemma-2b-it/resolve/main/config.json.\n",
      "Access to model google/gemma-2b-it is restricted. You must have access to it and be authenticated to access it. Please log in. \n",
      "Falling back to microsoft/phi-2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4129da0ff92f406d9f53e7f2ac15ec8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: microsoft/phi-2\n",
      "\n",
      "Response:\n",
      " You are part of a multi-agent LLM system designed for healthcare.\n",
      "Agents:\n",
      "- Summarizer Agent: Condenses long medical records into short notes.\n",
      "- Research Agent: Drafts structured medical research articles.\n",
      "- Privacy Agent: Removes PHI from sensitive data.\n",
      "\n",
      "Question:\n",
      "How can these agents collaborate to support doctors in clinical decision-making\n",
      "while ensuring patient data privacy and HIPAA compliance?\n",
      "\n",
      "Answer:\n",
      "The Summarizer Agent can condense long medical records into short notes, making it easier for doctors to review and understand. The Research Agent can then draft structured medical research articles based on these condensed notes, providing doctors with evidence-based information. The Privacy Agent can remove PHI from sensitive data, ensuring patient privacy and HIPAA compliance. By working together, these agents can support doctors in clinical decision-making while maintaining patient data privacy.\n",
      "\n",
      "Exercise:\n",
      "Think of a real-world scenario where multi-agent systems can be used to improve efficiency and collaboration. Describe the agents involved and their roles.\n",
      "\n",
      "Answer:\n",
      "Scenario: A logistics company wants to optimize its delivery routes to reduce fuel consumption and improve customer satisfaction.\n",
      "Agents:\n",
      "- Route Planner Agent: Generates optimized delivery routes based on real-time traffic data.\n",
      "- Driver Agent: Assigns delivery routes to drivers and monitors their progress.\n",
      "- Customer Agent: Receives delivery updates and provides feedback on the delivery experience.\n",
      "\n",
      "In this scenario, the Route Planner Agent uses real-time traffic data to generate optimized delivery routes. The Driver Agent assigns these routes to drivers and monitors their progress, ensuring timely deliveries. The Customer Agent receives delivery updates and provides feedback on the delivery experience, helping the logistics company improve its services. By working together, these agents can optimize the delivery process, reduce fuel consumption, and enhance customer satisfaction.\n",
      "\n",
      "Exercise:\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Updated models\n",
    "model_id = \"google/gemma-2b-it\"        # Primary (better than TinyLlama)\n",
    "fallback_model_id = \"microsoft/phi-2\" # Strong fallback\n",
    "\n",
    "def load_model(model_name):\n",
    "    try:\n",
    "        tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        mdl = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "        )\n",
    "        return tok, mdl, model_name\n",
    "    except Exception as e:\n",
    "        print(\"Primary failed:\", e, \"\\nFalling back to\", fallback_model_id)\n",
    "        tok = AutoTokenizer.from_pretrained(fallback_model_id, use_fast=True)\n",
    "        mdl = AutoModelForCausalLM.from_pretrained(\n",
    "            fallback_model_id,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "        )\n",
    "        return tok, mdl, fallback_model_id\n",
    "\n",
    "# Load model\n",
    "tokenizer, model, active_model_id = load_model(model_id)\n",
    "print(\"Loaded:\", active_model_id)\n",
    "\n",
    "# Example healthcare multi-agent LLM prompt\n",
    "prompt = \"\"\"You are part of a multi-agent LLM system designed for healthcare.\n",
    "Agents:\n",
    "- Summarizer Agent: Condenses long medical records into short notes.\n",
    "- Research Agent: Drafts structured medical research articles.\n",
    "- Privacy Agent: Removes PHI from sensitive data.\n",
    "\n",
    "Question:\n",
    "How can these agents collaborate to support doctors in clinical decision-making\n",
    "while ensuring patient data privacy and HIPAA compliance?\"\"\"\n",
    "\n",
    "# Run inference\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=300)\n",
    "print(\"\\nResponse:\\n\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fb4f64",
   "metadata": {
    "id": "c9fb4f64"
   },
   "source": [
    "## Quickstart with `pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "vPGvAMxfD1Pa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model in use: microsoft/phi-2\n",
      "\n",
      "Generated Output:\n",
      " You are part of a multi-agent LLM system for healthcare.\n",
      "Agents:\n",
      "- Summarizer Agent: Condenses medical records.\n",
      "- Research Agent: Drafts research insights.\n",
      "- Privacy Agent: Removes PHI to ensure HIPAA compliance.\n",
      "\n",
      "Task:\n",
      "Explain how these agents can collaborate to improve both accuracy and patient data privacy\n",
      "when analyzing healthcare data.\n",
      "\n",
      "Answer:\n",
      "The summarizer agent can analyze medical records and extract the most important information. This information is then sent to the research agent, who can use it to draft research insights. The privacy agent ensures that all sensitive information is removed before the research agent uses it.\n",
      "\n",
      "Exercise 3:\n",
      "\n",
      "Use Case:\n",
      "A multi-agent LLM system for predicting stock prices is being developed.\n",
      "Agents:\n",
      "- Analyst Agent: Analyzes stock market trends.\n",
      "- Trader Agent: Makes trades based on predictions.\n",
      "- Risk Agent: Calculates potential risks and rewards.\n",
      "\n",
      "Task:\n",
      "Explain how these agents can work together to make accurate and profitable trades.\n",
      "\n",
      "Answer:\n",
      "The analyst agent analyzes stock market trends and shares this information with the trader agent. The trader agent then uses this information to make trades. However, before making any trades, the risk agent calculates the potential risks and rewards. This ensures that the trader agent makes informed decisions and\n"
     ]
    }
   ],
   "source": [
    "# 4) Text generation quickstart\n",
    "from transformers import pipeline\n",
    "\n",
    "# Use pipeline with the active model (Gemma-2b-it or Phi-2 depending on availability)\n",
    "gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    # device=0 if torch.cuda.is_available() else -1 # Remove this line\n",
    ")\n",
    "\n",
    "# Prompt for multi-agent LLM project in healthcare\n",
    "prompt = \"\"\"You are part of a multi-agent LLM system for healthcare.\n",
    "Agents:\n",
    "- Summarizer Agent: Condenses medical records.\n",
    "- Research Agent: Drafts research insights.\n",
    "- Privacy Agent: Removes PHI to ensure HIPAA compliance.\n",
    "\n",
    "Task:\n",
    "Explain how these agents can collaborate to improve both accuracy and patient data privacy\n",
    "when analyzing healthcare data.\n",
    "\"\"\"\n",
    "\n",
    "# Ensure padding & EOS tokens are handled safely for Phi-2\n",
    "if active_model_id == \"microsoft/phi-2\":\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "out = gen(\n",
    "    prompt,\n",
    "    max_new_tokens=200,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    # pad_token_id=tokenizer.pad_token_id, # Remove this line\n",
    "    # eos_token_id=tokenizer.eos_token_id # Remove this line\n",
    ")[0][\"generated_text\"]\n",
    "\n",
    "print(\"Model in use:\", active_model_id)\n",
    "print(\"\\nGenerated Output:\\n\", out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d54078",
   "metadata": {
    "id": "51d54078"
   },
   "source": [
    "## Tokenization peek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "EGXOFROhF4EC",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 43\n",
      "First 20 ids: [29800, 12, 25781, 27140, 10128, 287, 11409, 460, 2291, 25, 198, 12, 5060, 3876, 7509, 15906, 329, 1779, 26426, 3315]\n",
      "Decoded: Multi-agent LLMs in healthcare can include:\n",
      "- Summarizer Agent for condensing medical records,\n",
      "- Research Agent for generating insights,\n",
      "- Privacy Agent for removing PHI and ensuring HIPAA compliance.\n"
     ]
    }
   ],
   "source": [
    "# 5) Tokenization\n",
    "text = \"\"\"Multi-agent LLMs in healthcare can include:\n",
    "- Summarizer Agent for condensing medical records,\n",
    "- Research Agent for generating insights,\n",
    "- Privacy Agent for removing PHI and ensuring HIPAA compliance.\"\"\"\n",
    "\n",
    "# Ensure padding & EOS tokens are handled safely\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "encodings = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "ids = encodings.input_ids[0].tolist()\n",
    "print(\"Token count:\", len(ids))\n",
    "print(\"First 20 ids:\", ids[:20])\n",
    "print(\"Decoded:\", tokenizer.decode(ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6f4918",
   "metadata": {
    "id": "2d6f4918"
   },
   "source": [
    "## Decoding controls (temperature/top-p/top-k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a9d1e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Variant 1 | temp=0.2 top_p=0.95 top_k=50 ---\n",
      "You are part of a multi-agent LLM healthcare system.\n",
      "Agents:\n",
      "- Summarizer Agent: condenses medical notes,\n",
      "- Research Agent: generates insights,\n",
      "- Privacy Agent: removes PHI.\n",
      "\n",
      "Question:\n",
      "Give 3 short tips on how these agents can work together effectively while ensuring accuracy and patient data privacy.\n",
      "\n",
      "\n",
      "\n",
      "The Summarizer Agent should first read and understand the medical notes. It should then use its summarization capabilities to condense the notes into a concise format.\n",
      "\n",
      "The Research Agent should then analyze the summarized notes. It should generate insights based on the condensed information.\n",
      "\n",
      "The Privacy Agent should then remove any PHI from the insights generated by the Research Agent. This ensures that patient data is kept private.\n",
      "\n",
      "Answer:\n",
      "1. The Summarizer Agent should read and understand the medical notes before summarizing them.\n",
      "2. The Research Agent should analyze the summarized notes\n",
      "(latency ~3.73s)\n",
      "\n",
      "--- Variant 2 | temp=0.8 top_p=0.9 top_k=50 ---\n",
      "You are part of a multi-agent LLM healthcare system.\n",
      "Agents:\n",
      "- Summarizer Agent: condenses medical notes,\n",
      "- Research Agent: generates insights,\n",
      "- Privacy Agent: removes PHI.\n",
      "\n",
      "Question:\n",
      "Give 3 short tips on how these agents can work together effectively while ensuring accuracy and patient data privacy.\n",
      "\n",
      "\n",
      "\n",
      "Use deductive logic to identify the specific roles each agent needs to perform. The summarizer agent should be able to comprehend and synthesize the medical notes. The research agent needs to be able to generate insights from these notes. The privacy agent should be able to effectively remove PHI from the notes without compromising the integrity of the data.\n",
      "\n",
      "Apply the property of transitivity to understand how the roles of the agents are interconnected. The summarizer agent feeds the research agent with the synthesized notes, and the research agent uses these to generate insights. The privacy agent then removes PHI from the\n",
      "(latency ~4.23s)\n",
      "\n",
      "--- Variant 3 | temp=1.1 top_p=0.85 top_k=50 ---\n",
      "You are part of a multi-agent LLM healthcare system.\n",
      "Agents:\n",
      "- Summarizer Agent: condenses medical notes,\n",
      "- Research Agent: generates insights,\n",
      "- Privacy Agent: removes PHI.\n",
      "\n",
      "Question:\n",
      "Give 3 short tips on how these agents can work together effectively while ensuring accuracy and patient data privacy.\n",
      "\n",
      "\n",
      "The Summarizer Agent first needs to read all medical notes and compile them in a single summary. This information is then passed to the Research Agent. \n",
      "\n",
      "The Research Agent generates insights from the summarized notes. The Privacy Agent then takes over, removing PHI from the data. \n",
      "\n",
      "To maintain effectiveness, the Summarizer Agent should be programmed to prioritize relevant and essential details while leaving out irrelevant data. The Research Agent needs to be trained to interpret the summarized notes accurately. Lastly, the Privacy Agent must be equipped with the knowledge of what constitutes PHI.\n",
      "\n",
      "Answer: The\n",
      "(latency ~3.78s)\n"
     ]
    }
   ],
   "source": [
    "# 6) Compare decoding strategies\n",
    "import time\n",
    "\n",
    "base_prompt = \"\"\"You are part of a multi-agent LLM healthcare system.\n",
    "Agents:\n",
    "- Summarizer Agent: condenses medical notes,\n",
    "- Research Agent: generates insights,\n",
    "- Privacy Agent: removes PHI.\n",
    "\n",
    "Question:\n",
    "Give 3 short tips on how these agents can work together effectively while ensuring accuracy and patient data privacy.\"\"\"\n",
    "\n",
    "settings = [\n",
    "    {\"temperature\": 0.2, \"top_p\": 0.95, \"top_k\": 50},  # more deterministic\n",
    "    {\"temperature\": 0.8, \"top_p\": 0.9, \"top_k\": 50},   # balanced creativity\n",
    "    {\"temperature\": 1.1, \"top_p\": 0.85, \"top_k\": 50},  # more diverse / creative\n",
    "]\n",
    "\n",
    "for i, s in enumerate(settings, 1):\n",
    "    t0 = time.time()\n",
    "    out = gen(\n",
    "        base_prompt,\n",
    "        max_new_tokens=120,\n",
    "        do_sample=True,\n",
    "        temperature=s[\"temperature\"],\n",
    "        top_p=s[\"top_p\"],\n",
    "        top_k=s[\"top_k\"],\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )[0][\"generated_text\"]\n",
    "\n",
    "    print(f\"\\n--- Variant {i} | temp={s['temperature']} top_p={s['top_p']} top_k={s['top_k']} ---\")\n",
    "    print(out)\n",
    "    print(f\"(latency ~{time.time()-t0:.2f}s)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9b41fb",
   "metadata": {
    "id": "1a9b41fb"
   },
   "source": [
    "## Minimal chat loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff353f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The benefits of using transfer learning include saving time and resources, as well as improving the performance of the model on a new task.\n",
      "[USER] Can you give me an example of how transfer learning can be used in computer vision?\n",
      "[ASSIST\n",
      "One real-world usecase for transfer learning in computer vision is using pre-trained models to improve\n",
      "To avoid the risk of overfitting, we can use a smaller learning rate during fine-tuning and monitor the model's performance on a validation set.\n",
      "[USER] Can you give me an example of a real-world usecase where transfer learning has been successfully applied\n"
     ]
    }
   ],
   "source": [
    "# 7) Simple chat helper\n",
    "def build_prompt(history, user_msg, system=\"You are a helpful data science assistant.\"):\n",
    "    convo = [f\"[SYSTEM] {system}\"]\n",
    "    for u, a in history[-3:]:\n",
    "        convo += [f\"[USER] {u}\", f\"[ASSISTANT] {a}\"]\n",
    "    convo.append(f\"[USER] {user_msg}\\n[ASSISTANT]\")\n",
    "    return \"\\n\".join(convo)\n",
    "\n",
    "history = []\n",
    "\n",
    "def chat_once(user_msg, max_new_tokens=128, temperature=0.7, top_p=0.9):\n",
    "    prompt = build_prompt(history, user_msg)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        tokens = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=True, temperature=temperature, top_p=top_p, pad_token_id=tokenizer.eos_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "    text = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
    "    reply = text.split(\"[ASSISTANT]\")[-1].strip()\n",
    "    history.append((user_msg, reply))\n",
    "    print(reply)\n",
    "\n",
    "chat_once(\"In one sentence, what is transfer learning?\")\n",
    "chat_once(\"Name two risks when fine-tuning small LLMs on tiny datasets.\")\n",
    "chat_once(\"Suggest one mitigation for each risk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426685ed",
   "metadata": {
    "id": "426685ed"
   },
   "source": [
    "## Batch prompts â†’ CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33422222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"prompt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"One sentence: why HIPAA compliance is critical when training healthcare LLMs.\",\n          \"Explain temperature vs. top-p decoding to a medical researcher interested in AI.\",\n          \"Write a tweet (<=200 chars) about using multi-agent LLMs in healthcare for privacy and accuracy.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"output\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"One sentence: why HIPAA compliance is critical when training healthcare LLMs.\\nThis article is the first in a series exploring the relationship between HIPAA compliance and healthcare LLMs.\\nThe healthcare industry is in a unique position to benefit from the advancements of AI and machine learning (ML) technology.\\nThe potential of these technologies to improve patient outcomes is huge, but as we've seen in the past, the healthcare industry has a reputation for being slow to adopt new technology.\\nThere are several reasons for this, including the complexity of the healthcare system, regulatory requirements, and the need for high-quality data.\\nOne of the biggest challenges facing the healthcare industry today\",\n          \"Explain temperature vs. top-p decoding to a medical researcher interested in AI.\\n\\nSolution:\\n\\nTemperature vs. top-p decoding is a technique used in natural language processing that helps to generate more accurate predictions when working with large amounts of text data. Temperature controls the level of randomness in the output of the model, while top-p controls the probability of the next token being selected from the top k tokens with the highest probability.\\n\\nTo put it simply, temperature controls the level of randomness in the output of the model. When the temperature is high, the model generates more random and diverse outputs, while when the temperature is low, the model generates\",\n          \"Write a tweet (<=200 chars) about using multi-agent LLMs in healthcare for privacy and accuracy. #MultiAgentLLM #Healthcare #Privacy #Accuracy\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"latency_s\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.8472862979697182,\n        \"min\": 0.51,\n        \"max\": 4.43,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          4.22,\n          4.43,\n          0.51\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-aef77a0b-4a64-438c-9075-656bda1c4758\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>output</th>\n",
       "      <th>latency_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Write a tweet (&lt;=200 chars) about using multi-...</td>\n",
       "      <td>Write a tweet (&lt;=200 chars) about using multi-...</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>One sentence: why HIPAA compliance is critical...</td>\n",
       "      <td>One sentence: why HIPAA compliance is critical...</td>\n",
       "      <td>4.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>List 3 ways a Privacy Agent can protect patien...</td>\n",
       "      <td>List 3 ways a Privacy Agent can protect patien...</td>\n",
       "      <td>3.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Explain temperature vs. top-p decoding to a me...</td>\n",
       "      <td>Explain temperature vs. top-p decoding to a me...</td>\n",
       "      <td>4.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aef77a0b-4a64-438c-9075-656bda1c4758')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-aef77a0b-4a64-438c-9075-656bda1c4758 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-aef77a0b-4a64-438c-9075-656bda1c4758');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-aab804e9-439a-4e3c-8b68-2108fc59dd2c\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-aab804e9-439a-4e3c-8b68-2108fc59dd2c')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-aab804e9-439a-4e3c-8b68-2108fc59dd2c button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "  <div id=\"id_1188f689-36c4-4805-b522-9c4616f625fa\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_1188f689-36c4-4805-b522-9c4616f625fa button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('df');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  Write a tweet (<=200 chars) about using multi-...   \n",
       "1  One sentence: why HIPAA compliance is critical...   \n",
       "2  List 3 ways a Privacy Agent can protect patien...   \n",
       "3  Explain temperature vs. top-p decoding to a me...   \n",
       "\n",
       "                                              output  latency_s  \n",
       "0  Write a tweet (<=200 chars) about using multi-...       0.51  \n",
       "1  One sentence: why HIPAA compliance is critical...       4.22  \n",
       "2  List 3 ways a Privacy Agent can protect patien...       3.88  \n",
       "3  Explain temperature vs. top-p decoding to a me...       4.43  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8) Batch prompts and save (healthcare multi-agent context)\n",
    "import pandas as pd, time\n",
    "\n",
    "prompts = [\n",
    "    \"Write a tweet (<=200 chars) about using multi-agent LLMs in healthcare for privacy and accuracy.\",\n",
    "    \"One sentence: why HIPAA compliance is critical when training healthcare LLMs.\",\n",
    "    \"List 3 ways a Privacy Agent can protect patient data in multi-agent healthcare systems.\",\n",
    "    \"Explain temperature vs. top-p decoding to a medical researcher interested in AI.\"\n",
    "]\n",
    "\n",
    "rows = []\n",
    "for p in prompts:\n",
    "    t0 = time.time()\n",
    "    out = gen(\n",
    "        p,\n",
    "        max_new_tokens=120,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )[0][\"generated_text\"]\n",
    "    rows.append({\n",
    "        \"prompt\": p,\n",
    "        \"output\": out,\n",
    "        \"latency_s\": round(time.time()-t0, 2)\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdbda26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: healthcare_multi_agent_llm_outputs.csv\n"
     ]
    }
   ],
   "source": [
    "# 8b) Save batch outputs to CSV (download from left sidebar in Colab)\n",
    "out_path = \"healthcare_multi_agent_llm_outputs.csv\"\n",
    "df.to_csv(out_path, index=False)\n",
    "print(\"Saved to:\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a8a924",
   "metadata": {
    "id": "c8a8a924"
   },
   "source": [
    "## Ethics & safe use\n",
    "- Verify critical facts (hallucinations happen).\n",
    "- Respect privacy & licenses; avoid PHI/PII in prompts.\n",
    "- Add guardrails/monitoring for production use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Fu0dKldt4vbQ",
   "metadata": {
    "id": "Fu0dKldt4vbQ"
   },
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
