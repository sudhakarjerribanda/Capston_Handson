{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aE2_8MWj7iBU"
   },
   "source": [
    "# Week 6 â€“ Track C : Streamlit Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LqoISJzu7n-r"
   },
   "source": [
    "# Step 1 | Install and Prepare Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m437.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m109.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m116.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m137.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m136.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
      "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.3 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.3 which is incompatible.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.3 which is incompatible.\n",
      "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.3 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.3 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.3 which is incompatible.\n",
      "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mCollecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "# âœ… Force stable NumPy and install dependencies\n",
    "!pip install -U numpy==1.26.4 --quiet\n",
    "!pip install -qU google-generativeai streamlit faiss-cpu sentence-transformers spacy networkx matplotlib pandas python-docx PyMuPDF\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "veEeFsr87tR1"
   },
   "source": [
    "# Step 2 | Gemini API Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Gemini 2.5 Flash connected successfully.\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# âœ… Use your working Gemini API key\n",
    "GOOGLE_API_KEY = \"AIzaSyD38xnKP0Qj30ZEu1PKKpFBZH5TsH1RESg\"\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# âœ… Use a model that exists in Colab (checked earlier)\n",
    "model = genai.GenerativeModel(\"models/gemini-2.5-flash\")\n",
    "\n",
    "print(\"âœ… Gemini 2.5 Flash connected successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmjYDLn67_qm"
   },
   "source": [
    "# Step 3 | Create Streamlit App File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "import google.generativeai as genai\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np, faiss\n",
    "\n",
    "# --- Configuration ---\n",
    "genai.configure(api_key=\"AIzaSyD38xnKP0Qj30ZEu1PKKpFBZH5TsH1RESg\")\n",
    "model = genai.GenerativeModel(\"models/gemini-2.5-flash\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# --- Streamlit UI ---\n",
    "st.set_page_config(page_title=\"Next-Level RAG Demo\", layout=\"wide\")\n",
    "st.title(\"ğŸ” Graph-RAG + Multi-Hop Reasoning App\")\n",
    "\n",
    "uploaded_files = st.file_uploader(\"ğŸ“‚ Upload project files\", type=[\"pdf\", \"docx\", \"txt\"], accept_multiple_files=True)\n",
    "texts = []\n",
    "\n",
    "if uploaded_files:\n",
    "    import fitz, docx\n",
    "    for f in uploaded_files:\n",
    "        text = \"\"\n",
    "        if f.name.endswith(\".pdf\"):\n",
    "            with fitz.open(stream=f.read(), filetype=\"pdf\") as pdf:\n",
    "                for page in pdf:\n",
    "                    text += page.get_text()\n",
    "        elif f.name.endswith(\".docx\"):\n",
    "            d = docx.Document(f)\n",
    "            text = \"\\n\".join([p.text for p in d.paragraphs])\n",
    "        else:\n",
    "            text = f.read().decode(\"utf-8\")\n",
    "        texts.append(text)\n",
    "\n",
    "    st.success(f\"âœ… Loaded {len(texts)} documents.\")\n",
    "\n",
    "    # Chunk text\n",
    "    chunks = []\n",
    "    for t in texts:\n",
    "        for i in range(0, len(t), 500):\n",
    "            c = t[i:i+500].strip()\n",
    "            if c:\n",
    "                chunks.append(c)\n",
    "    st.write(f\"ğŸ“‘ Total chunks: {len(chunks)}\")\n",
    "\n",
    "    # Embeddings + FAISS\n",
    "    embeddings = embedder.encode(chunks, convert_to_tensor=False)\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(np.array(embeddings))\n",
    "\n",
    "    # Graph building\n",
    "    G = nx.DiGraph()\n",
    "    for t in chunks:\n",
    "        doc = nlp(t)\n",
    "        ents = [e.text for e in doc.ents]\n",
    "        for i in range(len(ents) - 1):\n",
    "            G.add_edge(ents[i], ents[i+1], relation=\"related_to\")\n",
    "    st.write(f\"ğŸ§  Graph built: {len(G.nodes)} nodes, {len(G.edges)} edges.\")\n",
    "\n",
    "    # Query input\n",
    "    query = st.text_input(\"ğŸ” Enter your question\", \"Which author proposed Method B and which dataset did they evaluate it on?\")\n",
    "    if st.button(\"Run Query\"):\n",
    "        # --- Baseline Retrieval ---\n",
    "        q_emb = embedder.encode([query], convert_to_tensor=False)\n",
    "        D, I = index.search(np.array(q_emb), k=3)\n",
    "        context = \"\\n\".join([chunks[i] for i in I[0]])\n",
    "\n",
    "        baseline_prompt = f\"Using the context below, answer the question precisely.\\n\\n{context}\\n\\nQuestion: {query}\"\n",
    "        baseline_answer = model.generate_content(baseline_prompt).text.strip()\n",
    "\n",
    "        # --- Graph-RAG Retrieval ---\n",
    "        qdoc = nlp(query)\n",
    "        qents = [e.text for e in qdoc.ents]\n",
    "        neighborhood = []\n",
    "        for e in qents:\n",
    "            if e in G:\n",
    "                for n in G.neighbors(e):\n",
    "                    for t in chunks:\n",
    "                        if n in t:\n",
    "                            neighborhood.append(t)\n",
    "        graph_ctx = \"\\n\".join(neighborhood or chunks)\n",
    "\n",
    "        graph_prompt = f\"Answer the question using graph reasoning:\\n\\n{graph_ctx}\\n\\nQuestion: {query}\"\n",
    "        graph_answer = model.generate_content(graph_prompt).text.strip()\n",
    "\n",
    "        # --- Display results ---\n",
    "        st.subheader(\"ğŸ§© Baseline RAG Answer\")\n",
    "        st.write(baseline_answer)\n",
    "        st.subheader(\"ğŸ•¸ Graph-RAG Answer\")\n",
    "        st.write(graph_answer)\n",
    "\n",
    "        # --- Graph visualization ---\n",
    "        fig, ax = plt.subplots(figsize=(6, 5))\n",
    "        nx.draw(G, with_labels=False, node_color=\"skyblue\", node_size=40, edge_color=\"gray\", ax=ax)\n",
    "        st.pyplot(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
      "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.186.19.253:8501\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Stopping...\u001b[0m\n",
      "\u001b[34m  Stopping...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!streamlit run app.py --server.enableCORS false --server.enableXsrfProtection false --server.port 8501\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ Public URL: https://boyd-unribboned-noncontemptibly.ngrok-free.dev\n",
      "\n",
      "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
      "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.186.19.253:8501\u001b[0m\n",
      "\u001b[0m\n",
      "2025-10-07 04:36:32.927548: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1759811792.949104    2034 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1759811792.955981    2034 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1759811792.972827    2034 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1759811792.972857    2034 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1759811792.972862    2034 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1759811792.972865    2034 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-10-07 04:36:32.978251: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "modules.json: 100% 349/349 [00:00<00:00, 2.51MB/s]\n",
      "config_sentence_transformers.json: 100% 116/116 [00:00<00:00, 915kB/s]\n",
      "README.md: 10.5kB [00:00, 34.3MB/s]\n",
      "sentence_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 398kB/s]\n",
      "config.json: 100% 612/612 [00:00<00:00, 4.56MB/s]\n",
      "model.safetensors: 100% 90.9M/90.9M [00:00<00:00, 92.7MB/s]\n",
      "tokenizer_config.json: 100% 350/350 [00:00<00:00, 2.51MB/s]\n",
      "vocab.txt: 232kB [00:00, 17.7MB/s]\n",
      "tokenizer.json: 466kB [00:00, 45.2MB/s]\n",
      "special_tokens_map.json: 100% 112/112 [00:00<00:00, 1.05MB/s]\n",
      "config.json: 100% 190/190 [00:00<00:00, 1.25MB/s]\n"
     ]
    }
   ],
   "source": [
    "# --- Install and import pyngrok ---\n",
    "!pip install -q pyngrok streamlit\n",
    "\n",
    "from pyngrok import ngrok\n",
    "import time, os\n",
    "\n",
    "# --- Authenticate ngrok with your token ---\n",
    "ngrok.kill()  # close any previous tunnels\n",
    "ngrok.set_auth_token(\"33RDPz5AZYmujipffSg0KqV0lvV_7LSnVw5jucbapPGL7NyCC\")\n",
    "\n",
    "# --- Create a public tunnel for Streamlit (port 8501) ---\n",
    "public_url = ngrok.connect(8501).public_url\n",
    "print(\"ğŸŒ Public URL:\", public_url)\n",
    "\n",
    "# --- Run Streamlit app ---\n",
    "!streamlit run app.py --server.port 8501 --server.enableCORS false --server.enableXsrfProtection false &\n",
    "time.sleep(5)\n",
    "\n",
    "print(\"âœ… App is launching... click the link above to open it.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
