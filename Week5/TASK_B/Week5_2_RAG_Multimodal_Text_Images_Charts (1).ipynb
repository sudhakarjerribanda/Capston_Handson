{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Um_PC3oZx5ud"
   },
   "source": [
    "# ✅ Install dependencies & log environment\n",
    "This cell installs required libraries (Colab-friendly). It logs versions to `env_multimodal.json` for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"timestamp\": \"2025-09-28T00:51:10\",\n",
      "  \"python\": \"3.12.11\",\n",
      "  \"platform\": \"Linux-6.6.97+-x86_64-with-glibc2.35\",\n",
      "  \"numpy\": \"2.0.2\",\n",
      "  \"torch\": \"2.8.0+cu126\",\n",
      "  \"torch_cuda_available\": false,\n",
      "  \"device\": \"CPU\",\n",
      "  \"pillow\": \"11.3.0\",\n",
      "  \"sentence_transformers\": \"5.1.0\",\n",
      "  \"transformers\": \"4.56.1\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# (Run this in Colab/Jupyter; skip install if already set up)\n",
    "# NOTE: avoid force-uninstall to prevent breaking environment; only install what we need.\n",
    "!pip install -q transformers sentence-transformers faiss-cpu Pillow langchain pypdf ipywidgets\n",
    "\n",
    "import sys, platform, json, os, pathlib, datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import PIL\n",
    "import sentence_transformers, transformers\n",
    "\n",
    "env = {\n",
    "    \"timestamp\": datetime.datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"python\": sys.version.split()[0],\n",
    "    \"platform\": platform.platform(),\n",
    "    \"numpy\": np.__version__,\n",
    "    \"torch\": torch.__version__,\n",
    "    \"torch_cuda_available\": torch.cuda.is_available(),\n",
    "    \"device\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\",\n",
    "    \"pillow\": PIL.__version__,\n",
    "    \"sentence_transformers\": sentence_transformers.__version__,\n",
    "    \"transformers\": transformers.__version__,\n",
    "}\n",
    "print(json.dumps(env, indent=2))\n",
    "with open(\"env_multimodal.json\",\"w\") as f:\n",
    "    json.dump(env, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrwEtwEQyHui"
   },
   "source": [
    "# 🗂 Prepare folders and upload your project documents and images\n",
    "Upload: clinical papers, research drafts, notes (.txt/.md/.pdf) and images/charts (.png/.jpg).\n",
    "If running in Colab, you'll be prompted to upload. In classic Jupyter, drop files into the working folder or use the widget.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab detected. Please upload images (multi-select).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-c4d60685-2a29-425e-b13e-4f4b87c9022a\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-c4d60685-2a29-425e-b13e-4f4b87c9022a\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ablation1.png to ablation1 (1).png\n",
      "Saving ablation2.png to ablation2 (1).png\n",
      "Saving ablation4.png to ablation4 (1).png\n",
      "Saving BDI dialog (1)-1.png to BDI dialog (1)-1 (1).png\n",
      "Saving comparison.png to comparison (1).png\n",
      "Saving framework.png to framework (1).png\n",
      "Saving trust game plot.png to trust game plot (1).png\n",
      "Saving unnamed (1).png to unnamed (1) (1).png\n",
      "Saving unnamed (2).png to unnamed (2) (1).png\n",
      "Saving unnamed.png to unnamed (3).png\n",
      "\n",
      "(Optional) Upload PDFs/TXTs (or skip):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-7143ccec-0dde-4257-96dd-4b5485a9642f\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-7143ccec-0dde-4257-96dd-4b5485a9642f\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 1_NeurIPS-2024-mdagents-an-adaptive-collaboration-of-llms-for-medical-decision-making-Paper-Conference.pdf to 1_NeurIPS-2024-mdagents-an-adaptive-collaboration-of-llms-for-medical-decision-making-Paper-Conference (2).pdf\n",
      "Saving 2_NeurIPS-2024-richelieu-self-evolving-llm-based-agents-for-ai-diplomacy-Paper-Conference.pdf to 2_NeurIPS-2024-richelieu-self-evolving-llm-based-agents-for-ai-diplomacy-Paper-Conference (2).pdf\n",
      "Saving 3_NeurIPS-2024-can-large-language-model-agents-simulate-human-trust-behavior-Paper-Conference.pdf to 3_NeurIPS-2024-can-large-language-model-agents-simulate-human-trust-behavior-Paper-Conference (2).pdf\n",
      "Saved images: ['BDI dialog (1)-1 (1).png', 'BDI dialog (1)-1.png', 'ablation1 (1).png', 'ablation1.png', 'ablation2 (1).png', 'ablation2.png', 'ablation4 (1).png', 'ablation4.png', 'comparison (1).png', 'comparison.png', 'framework (1).png', 'framework.png', 'trust game plot (1).png', 'trust game plot.png', 'unnamed (1) (1).png', 'unnamed (1).png', 'unnamed (2) (1).png', 'unnamed (2).png', 'unnamed (3).png', 'unnamed.png']\n",
      "Saved text files: ['1_NeurIPS-2024-mdagents-an-adaptive-collaboration-of-llms-for-medical-decision-making-Paper-Conference (1).pdf', '1_NeurIPS-2024-mdagents-an-adaptive-collaboration-of-llms-for-medical-decision-making-Paper-Conference (2).pdf', '2_NeurIPS-2024-richelieu-self-evolving-llm-based-agents-for-ai-diplomacy-Paper-Conference (1).pdf', '2_NeurIPS-2024-richelieu-self-evolving-llm-based-agents-for-ai-diplomacy-Paper-Conference (2).pdf', '3_NeurIPS-2024-can-large-language-model-agents-simulate-human-trust-behavior-Paper-Conference (1).pdf', '3_NeurIPS-2024-can-large-language-model-agents-simulate-human-trust-behavior-Paper-Conference (2).pdf']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os, sys\n",
    "\n",
    "IMG_DIR = Path(\"mm_images\"); IMG_DIR.mkdir(exist_ok=True)\n",
    "TXT_DIR = Path(\"corpus\");    TXT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Helper to detect environment (Colab or Jupyter)\n",
    "def running_in_colab():\n",
    "    return \"google.colab\" in sys.modules\n",
    "\n",
    "if running_in_colab():\n",
    "    # Colab upload\n",
    "    from google.colab import files\n",
    "    print(\"Colab detected. Please upload images (multi-select).\")\n",
    "    up_imgs = files.upload()\n",
    "    for name, data in (up_imgs or {}).items():\n",
    "        if name.lower().endswith((\".png\",\".jpg\",\".jpeg\",\".webp\",\".bmp\",\".tif\",\".tiff\")):\n",
    "            (IMG_DIR / name).write_bytes(data)\n",
    "    print(\"\\n(Optional) Upload PDFs/TXTs (or skip):\")\n",
    "    up_txts = files.upload()\n",
    "    for name, data in (up_txts or {}).items():\n",
    "        (TXT_DIR / name).write_bytes(data)\n",
    "else:\n",
    "    # Jupyter: show an upload widget (ipywidgets) — works in JupyterLab/Notebook\n",
    "    try:\n",
    "        import ipywidgets as widgets\n",
    "        from IPython.display import display\n",
    "        uploaded = {}\n",
    "\n",
    "        def on_upload_change(change):\n",
    "            for fname, fobj in uploader.value.items():\n",
    "                b = fobj['content']\n",
    "                (IMG_DIR / fname).write_bytes(b)\n",
    "            print(\"Saved uploaded images to mm_images/\")\n",
    "\n",
    "        print(\"If running in classic Jupyter, please upload images and text using the Jupyter file browser or place files into ./mm_images and ./corpus.\")\n",
    "        # Note: For simplicity, we don't create a multi-file widget here; instruct user to upload files manually.\n",
    "    except Exception:\n",
    "        print(\"No interactive upload widget available — please copy files into ./mm_images and ./corpus manually.\")\n",
    "\n",
    "print(\"Saved images:\", sorted([p.name for p in IMG_DIR.iterdir() if p.is_file()]))\n",
    "print(\"Saved text files:\", sorted([p.name for p in TXT_DIR.iterdir() if p.is_file()]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cik5yPtayWnh"
   },
   "source": [
    "# 📄 Load & chunk text documents\n",
    "We use langchain loaders for PDFs and plain text, then split into chunks suitable for embeddings (chunk_size=500, overlap=100).\n",
    "These chunks will become the text part of the multimodal index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.3.30)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=0.3.75 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.76)\n",
      "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.27)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.43)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.28)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (0.3.11)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (2.11.9)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (4.15.0)\n",
      "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (25.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.8.3)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<2.0.0,>=0.3.75->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (2.33.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain-community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Text chunks: 2132\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"display(text_corpus\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"doc_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"doc0\",\n          \"doc1\",\n          \"doc2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"corpus/1_NeurIPS-2024-mdagents-an-adaptive-collaboration-of-llms-for-medical-decision-making-Paper-Conference (1).pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"MDAgents: An Adaptive Collaboration of LLMs for\\nMedical Decision-Making\\nYubin Kim1 Chanwoo Park1 Hyewon Jeong1\\u266e Yik Siu Chan1\\nXuhai Xu1 Daniel McDuff2 Hyeonhoon Lee3\\nMarzyeh Ghassemi1 Cynthia Breazeal1 Hae Won Park1\\n1Massachusetts Institute of Technology\\n2Google Research\\n3Seoul National University Hospital\\n{ybkim95,cpark97,hyewonj,yiksiuc,xoxu,mghassem,cynthiab,haewon}@mit.edu\\ndmcduff@google.com\\nhhoon@snu.ac.kr\\nAbstract\\nFoundation models are becoming valuable tools in medicine. Yet despite their\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-d4d5c241-0df1-4f71-91d4-6f92d833628e\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc0</td>\n",
       "      <td>corpus/1_NeurIPS-2024-mdagents-an-adaptive-col...</td>\n",
       "      <td>MDAgents: An Adaptive Collaboration of LLMs fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc1</td>\n",
       "      <td>corpus/1_NeurIPS-2024-mdagents-an-adaptive-col...</td>\n",
       "      <td>Abstract\\nFoundation models are becoming valua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc2</td>\n",
       "      <td>corpus/1_NeurIPS-2024-mdagents-an-adaptive-col...</td>\n",
       "      <td>The assigned solo or group collaboration struc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d4d5c241-0df1-4f71-91d4-6f92d833628e')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-d4d5c241-0df1-4f71-91d4-6f92d833628e button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-d4d5c241-0df1-4f71-91d4-6f92d833628e');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-d7076923-a2c6-4231-a197-ad70e7aa54e9\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d7076923-a2c6-4231-a197-ad70e7aa54e9')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-d7076923-a2c6-4231-a197-ad70e7aa54e9 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "  doc_id                                             source  \\\n",
       "0   doc0  corpus/1_NeurIPS-2024-mdagents-an-adaptive-col...   \n",
       "1   doc1  corpus/1_NeurIPS-2024-mdagents-an-adaptive-col...   \n",
       "2   doc2  corpus/1_NeurIPS-2024-mdagents-an-adaptive-col...   \n",
       "\n",
       "                                                text  \n",
       "0  MDAgents: An Adaptive Collaboration of LLMs fo...  \n",
       "1  Abstract\\nFoundation models are becoming valua...  \n",
       "2  The assigned solo or group collaboration struc...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "\n",
    "docs = []\n",
    "for p in sorted(TXT_DIR.iterdir()):\n",
    "    if not p.is_file():\n",
    "        continue\n",
    "    ext = p.suffix.lower()\n",
    "    try:\n",
    "        if ext == \".pdf\":\n",
    "            docs.extend(PyPDFLoader(str(p)).load())\n",
    "        elif ext in [\".txt\", \".md\", \".text\"]:\n",
    "            docs.extend(TextLoader(str(p), encoding=\"utf-8\").load())\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not read {p.name}: {e}\")\n",
    "\n",
    "# If no user docs uploaded, provide a small medical-themed sample\n",
    "if not docs:\n",
    "    sample_text = (\n",
    "        \"This sample clinical note mentions Chart A (patient vitals over time) and Chart B (lab trends). \"\n",
    "        \"The project: build agents to summarize medical text, refine research articles, and sanitize PHI.\"\n",
    "    )\n",
    "    (TXT_DIR / \"sample_project_note.txt\").write_text(sample_text, encoding=\"utf-8\")\n",
    "    docs.extend(TextLoader(str(TXT_DIR / \"sample_project_note.txt\"), encoding=\"utf-8\").load())\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "text_corpus = pd.DataFrame({\n",
    "    \"doc_id\":   [f\"doc{i}\" for i in range(len(chunks))],\n",
    "    \"source\":   [c.metadata.get(\"source\") or c.metadata.get(\"file_path\") or \"uploaded\" for c in chunks],\n",
    "    \"text\":     [c.page_content for c in chunks],\n",
    "})\n",
    "print(\"✅ Text chunks:\", len(text_corpus))\n",
    "display(text_corpus.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZM7-Fxmz-m0"
   },
   "source": [
    "# Load Images & Auto-Caption (BLIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3oSsLRZZ0gDU"
   },
   "source": [
    "# 🖼️ Load images & auto-caption (BLIP)\n",
    "We caption images (charts/figures) using BLIP. If captioning fails, we fall back to filename.\n",
    "Captions will be indexed alongside text chunks for joint retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Images found: 20\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"        print(\\\"\\u26a0\\ufe0f Captioning not available, fallback to filenames\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"image_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"BDI dialog (1)-1 (1)\",\n          \"BDI dialog (1)-1\",\n          \"ablation1 (1)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"mm_images/BDI dialog (1)-1 (1).png\",\n          \"mm_images/BDI dialog (1)-1.png\",\n          \"mm_images/ablation1 (1).png\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"caption\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"BDI dialog (1)-1 (1)\",\n          \"BDI dialog (1)-1\",\n          \"ablation1 (1)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-9b5488ac-cbb0-497a-a31a-a2bdfcabd41a\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>path</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BDI dialog (1)-1 (1)</td>\n",
       "      <td>mm_images/BDI dialog (1)-1 (1).png</td>\n",
       "      <td>BDI dialog (1)-1 (1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BDI dialog (1)-1</td>\n",
       "      <td>mm_images/BDI dialog (1)-1.png</td>\n",
       "      <td>BDI dialog (1)-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ablation1 (1)</td>\n",
       "      <td>mm_images/ablation1 (1).png</td>\n",
       "      <td>ablation1 (1)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9b5488ac-cbb0-497a-a31a-a2bdfcabd41a')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-9b5488ac-cbb0-497a-a31a-a2bdfcabd41a button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-9b5488ac-cbb0-497a-a31a-a2bdfcabd41a');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-fe867330-696a-4e1c-a887-ad57efdebc04\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fe867330-696a-4e1c-a887-ad57efdebc04')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-fe867330-696a-4e1c-a887-ad57efdebc04 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "               image_id                                path  \\\n",
       "0  BDI dialog (1)-1 (1)  mm_images/BDI dialog (1)-1 (1).png   \n",
       "1      BDI dialog (1)-1      mm_images/BDI dialog (1)-1.png   \n",
       "2         ablation1 (1)         mm_images/ablation1 (1).png   \n",
       "\n",
       "                caption  \n",
       "0  BDI dialog (1)-1 (1)  \n",
       "1      BDI dialog (1)-1  \n",
       "2         ablation1 (1)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2838f67c086e4ab3867ed7c45d51bcf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d106d42db7334f86a3babbfafdd79bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6ccf278de74522895d2d332576ab1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b61be355404c1f9f2d88a9e88f5ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59f9d4e722744f0ab5c10131becd9b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c88684111f6f4b49801bcf03eff281aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a618e5d095994e6cb697971ff5cd8665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e8aa5a17e474397a95b077fc1f7df05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc39da94441449d28a6a4e8ece128c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Auto-captions generated.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "# Collect images\n",
    "img_rows = []\n",
    "for p in sorted(IMG_DIR.iterdir()):\n",
    "    if p.suffix.lower() in [\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\", \".tif\", \".tiff\"]:\n",
    "        img_rows.append({\"image_id\": p.stem, \"path\": str(p), \"caption\": p.stem})\n",
    "df_imgs = pd.DataFrame(img_rows)\n",
    "print(\"✅ Images found:\", len(df_imgs))\n",
    "display(df_imgs.head(3) if len(df_imgs) else df_imgs)\n",
    "\n",
    "# Auto-caption with BLIP\n",
    "CAPTION = True\n",
    "if CAPTION and len(df_imgs):\n",
    "    try:\n",
    "        processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        caption_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        new_caps = []\n",
    "        for row in df_imgs.itertuples():\n",
    "            try:\n",
    "                img = Image.open(row.path).convert(\"RGB\")\n",
    "                inputs = processor(img, return_tensors=\"pt\")\n",
    "                out = caption_model.generate(**inputs, max_new_tokens=32)\n",
    "                cap = processor.decode(out[0], skip_special_tokens=True)\n",
    "            except Exception:\n",
    "                cap = row.caption\n",
    "            new_caps.append(cap)\n",
    "        df_imgs[\"caption\"] = new_caps\n",
    "        print(\"📝 Auto-captions generated.\")\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ Captioning not available, fallback to filenames. Reason:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbiFvShc03ah"
   },
   "source": [
    "# 🔗 Merge text chunks and image captions\n",
    "We produce `all_docs` that contains both text chunks and image-caption items with consistent keys:\n",
    "- id: doc id or img_{filename}\n",
    "- text: chunk text or caption\n",
    "- meta: source / file path / image file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items to index (text + images): 2152\n"
     ]
    }
   ],
   "source": [
    "all_docs = []\n",
    "\n",
    "# Add text chunks\n",
    "for _, r in text_corpus.iterrows():\n",
    "    all_docs.append({\"id\": r[\"doc_id\"], \"text\": r[\"text\"], \"meta\": {\"source\": r[\"source\"], \"type\": \"text\"}})\n",
    "\n",
    "# Add images as short text docs (captions)\n",
    "for _, r in df_imgs.iterrows():\n",
    "    all_docs.append({\"id\": f\"img_{r['image_id']}\", \"text\": r.get(\"caption\", r[\"image_id\"]), \"meta\": {\"file\": r.get(\"path\"), \"type\":\"image\"}})\n",
    "\n",
    "print(\"Total items to index (text + images):\", len(all_docs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8hbks8E086i"
   },
   "source": [
    "# 🧠 Embeddings\n",
    "- `all-MiniLM-L6-v2` for semantic text embeddings (fast, good for text→text).\n",
    "- `clip-ViT-B-32` (SentenceTransformers wrapper) for image↔text cross-search (images encoded as images, texts encoded to CLIP space).\n",
    "We normalize embeddings for cosine (dot product after normalization).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51707294ebf0458c904ad39c599b868e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c14ba4d9584bf3ba950bb5e0db9624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aabe4b3b44d94a30826f822a44684763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dee6cdeb3534e829f5e742798b6379a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce4d91ce8c364a0b820de26d43f77c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77cbdb8fcdec45aeb0b46892b8661fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af162d8e25514763ba352c336b0d71e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb299a3d6d04a99a73607e82c179c85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d8e743282840739675b33cb84d692d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e3eba4c0f14745bf162fe549f67d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5114affc719046f5b87f7d6911e57e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "233719f5304d42539d8636a6c415a7c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5382c5abf3c54915bca34faad4ac6d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d0825578f9546ab8b13efed772130a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf3c39f8dcf408998ed9c1633e1e19c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d069fa3a4d4a83966dc238091b4827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a2a4196a044e2396899dc2920d2701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a5c8569e094bc191b9bbd2decb4d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a391972ddc42e091b04d33f5f7d967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/604 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "443260aeb4b5413dad0dc5b3520b6e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0_CLIPModel/model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d95ed63308f64be28491b05e816c7fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7642c20160a24b90bb8a0a5a1d1d1fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0_CLIPModel/pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d556831f9c54f5bb9407150ff540632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f44a242aca80462c89d8902353970d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3264f7bae049378813116ca2f59328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: minilm_text: (2152, 384) clip_text: (2152, 512) clip_images: (20, 512)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Text embedding model (MiniLM)\n",
    "st_text = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# CLIP model for joint image/text space\n",
    "st_clip = SentenceTransformer(\"clip-ViT-B-32\")\n",
    "\n",
    "# Prepare lists\n",
    "texts_for_minilm = [d[\"text\"] for d in all_docs]  # used for text->text (minilm)\n",
    "texts_for_clip   = [d[\"text\"] for d in all_docs]  # used for clip text encoding (for cross search)\n",
    "image_paths = [d[\"meta\"].get(\"file\") for d in all_docs if d[\"meta\"].get(\"type\")==\"image\"]\n",
    "\n",
    "# Encode text with MiniLM\n",
    "text_vecs_minilm = st_text.encode(texts_for_minilm, batch_size=32, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=True)\n",
    "\n",
    "# Encode texts with CLIP (text side)\n",
    "text_vecs_clip = st_clip.encode(texts_for_clip, batch_size=32, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=True)\n",
    "\n",
    "# Encode images with CLIP (image side) — we need the PIL images for those entries\n",
    "pil_images = []\n",
    "for d in all_docs:\n",
    "    if d[\"meta\"].get(\"type\")==\"image\":\n",
    "        try:\n",
    "            pil_images.append(Image.open(d[\"meta\"][\"file\"]).convert(\"RGB\"))\n",
    "        except Exception:\n",
    "            # placeholder if file not available\n",
    "            pil_images.append(Image.new(\"RGB\",(224,224), color=(255,255,255)))\n",
    "img_vecs_clip = st_clip.encode(pil_images, batch_size=16, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=True) if len(pil_images) else np.zeros((0, text_vecs_clip.shape[1]), dtype=\"float32\")\n",
    "\n",
    "print(\"Shapes: minilm_text:\", text_vecs_minilm.shape, \"clip_text:\", text_vecs_clip.shape, \"clip_images:\", img_vecs_clip.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIImAC5-1Eth"
   },
   "source": [
    "# 🔎 Retrieval helper functions\n",
    "Functions:\n",
    "- topk_cosine: fast dot-product ranking on normalized vectors\n",
    "- retrieve_text_by_text(query): text->text using MiniLM\n",
    "- retrieve_images_by_text(query): text->images using CLIP\n",
    "- retrieve_by_image(image_id): image->(text, images) via CLIP image vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text->Text sample: [('doc165', 0.739983320236206, 'Nidhi Rohatgi, Poonam Hosamani, William Collins, Neera Ahuja, Curtis P. Langlotz, Jason\\nHom, Sergios Gatidis, John Pauly, and Akshay S. Chaudhari. Adapted large language models can\\noutperform medical experts in clinical text summarization. Nature Medicine, 30(4):1134–1142,\\nFebruary 2024.\\n[80] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen,\\nJiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. A survey'), ('doc554', 0.739983320236206, 'Nidhi Rohatgi, Poonam Hosamani, William Collins, Neera Ahuja, Curtis P. Langlotz, Jason\\nHom, Sergios Gatidis, John Pauly, and Akshay S. Chaudhari. Adapted large language models can\\noutperform medical experts in clinical text summarization. Nature Medicine, 30(4):1134–1142,\\nFebruary 2024.\\n[80] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen,\\nJiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. A survey'), ('doc507', 0.5400217771530151, 'References\\n[1] Monica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim, and David Sontag. Large\\nlanguage models are few-shot clinical information extractors. arXiv preprint arXiv:2205.12689,\\n2022.\\n[2] Zaid Al-Ars, Obinna Agba, Zhuoran Guo, Christiaan Boerkamp, Ziyaad Jaber, and Tareq\\nJaber. Nlice: Synthetic medical record generation for effective primary healthcare differential\\ndiagnosis, 2024.\\n[3] Seongsu Bae, Daeun Kyung, Jaehee Ryu, Eunbyeol Cho, Gyubok Lee, Sunjun Kweon, Jungwoo')]\n",
      "Text->Images sample: [('img_trust game plot (1)', 0.25647494196891785, 'mm_images/trust game plot (1).png'), ('img_trust game plot', 0.25647494196891785, 'mm_images/trust game plot.png'), ('img_unnamed (2)', 0.22733423113822937, 'mm_images/unnamed (2).png')]\n",
      "Image->Docs/Images sample for first image: ([('doc1266', 0.29735758900642395, '• Researchers should communicate the details of the dataset/code/model as part of their\\nsubmissions via structured templates. This includes details about training, license,\\nlimitations, etc.\\n• The paper should discuss whether and how consent was obtained from people whose\\nasset is used.\\n• At submission time, remember to anonymize your assets (if applicable). You can either\\ncreate an anonymized URL or include an anonymized zip file.\\n14. Crowdsourcing and Research with Human Subjects'), ('doc1019', 0.2973574995994568, '• Researchers should communicate the details of the dataset/code/model as part of their\\nsubmissions via structured templates. This includes details about training, license,\\nlimitations, etc.\\n• The paper should discuss whether and how consent was obtained from people whose\\nasset is used.\\n• At submission time, remember to anonymize your assets (if applicable). You can either\\ncreate an anonymized URL or include an anonymized zip file.\\n14. Crowdsourcing and Research with Human Subjects'), ('doc734', 0.29328709840774536, 'perimental results of the paper to the extent that it affects the main claims and/or conclusions\\nof the paper (regardless of whether the code and data are provided or not)?\\nAnswer: [Yes]\\nJustification: The paper provides comprehensive details on datasets, experimental setups,\\nand methodologies used, ensuring that the results can be reproduced accurately.\\nGuidelines:\\n• The answer NA means that the paper does not include experiments.')], [('img_BDI dialog (1)-1', 0.9999998807907104, 'mm_images/BDI dialog (1)-1.png'), ('img_framework (1)', 0.718946099281311, 'mm_images/framework (1).png'), ('img_framework', 0.718946099281311, 'mm_images/framework.png')])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def topk_cosine(q_vec, mat, k=5):\n",
    "    if mat.shape[0] == 0:\n",
    "        return []\n",
    "    sims = mat @ q_vec\n",
    "    idx = np.argsort(-sims)[:k]\n",
    "    return [(int(i), float(sims[i])) for i in idx]\n",
    "\n",
    "# encoders\n",
    "def encode_text_minilm(q: str):\n",
    "    return st_text.encode([q], convert_to_numpy=True, normalize_embeddings=True)[0]\n",
    "\n",
    "def encode_text_clip(q: str):\n",
    "    return st_clip.encode([q], convert_to_numpy=True, normalize_embeddings=True)[0]\n",
    "\n",
    "def retrieve_text_by_text(query, k=5):\n",
    "    q = encode_text_minilm(query)\n",
    "    hits = topk_cosine(q, text_vecs_minilm, k=k)\n",
    "    return [(all_docs[i][\"id\"], hits[j][1], all_docs[i][\"text\"]) for j,(i,_) in enumerate(hits)]\n",
    "\n",
    "def retrieve_images_by_text(query, k=5):\n",
    "    if img_vecs_clip.shape[0] == 0: return []\n",
    "    q = encode_text_clip(query)\n",
    "    hits = topk_cosine(q, img_vecs_clip, k=k)\n",
    "    # map hit indices to all_docs image positions\n",
    "    image_doc_indices = [idx for idx,d in enumerate(all_docs) if d[\"meta\"].get(\"type\")==\"image\"]\n",
    "    return [(all_docs[image_doc_indices[i]][\"id\"], hits[j][1], all_docs[image_doc_indices[i]][\"meta\"].get(\"file\")) for j,(i,_) in enumerate(hits)]\n",
    "\n",
    "def retrieve_by_image(image_id, k=5):\n",
    "    # find index in all_docs\n",
    "    idx = next((i for i,d in enumerate(all_docs) if d[\"id\"]==f\"img_{image_id}\"), None)\n",
    "    if idx is None:\n",
    "        return [], []\n",
    "    # compute q_vec relative to img_vecs_clip ordering\n",
    "    # build mapping of image doc indices to img_vecs indices\n",
    "    image_doc_indices = [i for i,d in enumerate(all_docs) if d[\"meta\"].get(\"type\")==\"image\"]\n",
    "    img_idx_in_imgvecs = image_doc_indices.index(idx)\n",
    "    q_vec = img_vecs_clip[img_idx_in_imgvecs]\n",
    "    text_hits = topk_cosine(q_vec, text_vecs_clip, k=k)\n",
    "    img_hits  = topk_cosine(q_vec, img_vecs_clip,  k=k+1)  # may include itself\n",
    "    img_hits  = [(i,s) for (i,s) in img_hits if i!=img_idx_in_imgvecs][:k]\n",
    "    text_pairs = [(all_docs[i][\"id\"], text_hits[j][1], all_docs[i][\"text\"]) for j,(i,_) in enumerate(text_hits)]\n",
    "    img_pairs  = [(all_docs[image_doc_indices[i]][\"id\"], img_hits[j][1], all_docs[image_doc_indices[i]][\"meta\"].get(\"file\")) for j,(i,_) in enumerate(img_hits)]\n",
    "    return text_pairs, img_pairs\n",
    "\n",
    "# Quick tests (if content exists)\n",
    "if len(all_docs):\n",
    "    print(\"Text->Text sample:\", retrieve_text_by_text(\"summarize medical text\", k=3))\n",
    "    print(\"Text->Images sample:\", retrieve_images_by_text(\"patient vitals chart\", k=3))\n",
    "    if len(df_imgs):\n",
    "        print(\"Image->Docs/Images sample for first image:\", retrieve_by_image(df_imgs.iloc[0].image_id, k=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfFHXDW51N_L"
   },
   "source": [
    "# 🧩 Assemble RAG-style prompt for the generator\n",
    "The prompt instructs the generator to:\n",
    "- Use ONLY the evidence provided,\n",
    "- Cite sources inline using `[doc_id]` or `[img_id]`,\n",
    "- If evidence is insufficient, respond with a safe REFUSAL.\n",
    "This prompt will be passed to the generator for RAG answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_prompt(query, text_hits, image_hits, max_context_chars=4000):\n",
    "    tbits = []\n",
    "    for doc_id, score, fulltext in text_hits:\n",
    "        # fetch the text snippet (first 350 chars)\n",
    "        row_text = next((d for d in all_docs if d[\"id\"]==doc_id), {}).get(\"text\",\"\")\n",
    "        tbits.append(f\"[{doc_id}] {row_text[:350].replace('\\\\n',' ')}\")\n",
    "    ibits = []\n",
    "    for img_id, score, img_path in image_hits:\n",
    "        row = next((d for d in all_docs if d[\"id\"]==img_id), {})\n",
    "        ibits.append(f\"[{img_id}] {row.get('text','(image caption missing)')}\")\n",
    "    evidence = \"\"\n",
    "    if tbits:\n",
    "        evidence += \"Text:\\n\" + \"\\n\".join(tbits)\n",
    "    if ibits:\n",
    "        evidence += (\"\\n\\nImages:\\n\" + \"\\n\".join(ibits)) if evidence else \"\\n\".join(ibits)\n",
    "    prompt = (\n",
    "        \"System: You are an expert assistant for clinical research and medical writing. \"\n",
    "        \"Answer using ONLY the evidence below and include inline citations in the form [doc_id] or [img_id]. \"\n",
    "        \"If the evidence does not support a confident answer, reply exactly: REFUSE: insufficient reliable evidence.\\n\\n\"\n",
    "        f\"Query: {query}\\n\\nEvidence:\\n{evidence}\\n\\nAnswer:\"\n",
    "    )\n",
    "    if len(prompt) > max_context_chars:\n",
    "        # simple trim: keep first N chars of evidence\n",
    "        prompt = prompt[:max_context_chars]\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsJX4V3u3XB2"
   },
   "source": [
    "# ⚙️ Generator (RAG answer) — using an open generator (distilgpt2)\n",
    "We use `distilgpt2` here to avoid gated models. You may replace it with a larger instruction-tuned model if available.\n",
    "The wrapper checks for the presence of at least one inline citation in the generated text; otherwise we mark as refusal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ccaa7bd3944c5197e4c609ed5e1f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7057c084b92d4f44951fbccb1fb73d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e7d674e1ba8419fb30afee31e00b9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2923949790334be4bb43d6126567a70d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3044cdfcddcc4b629c56643577d9a61f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be66320f9e0f4c839d26ff8831d84bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1548e85b3734c5f971a5382ed5e51e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# Use an open model — distilgpt2\n",
    "GEN_MODEL = \"distilgpt2\"\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(GEN_MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(GEN_MODEL)\n",
    "# ensure pad token exists\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "gen_pipe = pipeline(\"text-generation\", model=model, tokenizer=tok, device=device, max_new_tokens=200)\n",
    "\n",
    "def generate_rag_answer(query, k_text=3, k_images=3):\n",
    "    t_hits = retrieve_text_by_text(query, k=k_text)\n",
    "    i_hits = retrieve_images_by_text(query, k=k_images)\n",
    "    prompt = assemble_prompt(query, t_hits, i_hits)\n",
    "    # quick guard: if no hits at all, refuse\n",
    "    if (not t_hits) and (not i_hits):\n",
    "        return {\"text\":\"REFUSE: insufficient reliable evidence.\", \"refused\":True, \"evidence\": {\"text\":t_hits, \"images\":i_hits}}\n",
    "    # generate\n",
    "    out = gen_pipe(prompt, do_sample=False)[0][\"generated_text\"]\n",
    "    # enforce presence of a citation token like [doc or [img\n",
    "    if (\"[doc\" not in out) and (\"[img\" not in out):\n",
    "        # If generator didn't cite, return refusal with evidence\n",
    "        return {\"text\":\"REFUSE: model did not include required citations.\", \"refused\":True, \"raw_output\": out, \"evidence\": {\"text\":t_hits, \"images\":i_hits}}\n",
    "    return {\"text\": out, \"refused\": False, \"evidence\": {\"text\":t_hits, \"images\":i_hits}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bVddvwX3dQh"
   },
   "source": [
    "# 🧪 Example queries for your project\n",
    "These queries are tailored to your project:\n",
    "- Summarize medical text\n",
    "- Refine research articles\n",
    "- Sanitize sensitive healthcare data (PHI)\n",
    "We show text-only, image-only, and hybrid RAG queries and the generator's output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Query: Summarize the key steps to sanitize PHI from a discharge summary.\n",
      "\n",
      "EVIDENCE (text ids): ['doc1216', 'doc969', 'doc507']\n",
      "EVIDENCE (image ids): ['img_BDI dialog (1)-1 (1)', 'img_BDI dialog (1)-1', 'img_ablation2']\n",
      "\n",
      "Result (refused?): False\n",
      "System: You are an expert assistant for clinical research and medical writing. Answer using ONLY the evidence below and include inline citations in the form [doc_id] or [img_id]. If the evidence does not support a confident answer, reply exactly: REFUSE: insufficient reliable evidence.\n",
      "\n",
      "Query: Summarize the key steps to sanitize PHI from a discharge summary.\n",
      "\n",
      "Evidence:\n",
      "Text:\n",
      "[doc1216] or the supplemental material, provided in the appendix. If you answer [Yes] to a question, in the\n",
      "justification please point to the section(s) where related material for the question can be found.\n",
      "IMPORTANT, please:\n",
      "• Delete this instruction block, but keep the section heading “NeurIPS paper checklist\",\n",
      "• Keep the checklist subsection headings, que\n",
      "[doc969] or the supplemental material, provided in the appendix. If you answer [Yes] to a question, in the\n",
      "justification please point to the section(s) where related material for the question can be found.\n",
      "IMPORTANT, please:\n",
      "• Delete this instruction block, but keep the section heading “NeurIPS paper checklist\",\n",
      "• Keep the checklist subsection headings, que\n",
      "[doc507] References\n",
      "[1] Monica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim, and David Sontag. Large\n",
      "language models are few-shot clinical information extractors. arXiv preprint arXiv:2205.12689,\n",
      "2022.\n",
      "[2] Zaid Al-Ars, Obinna Agba, Zhuoran Guo, Christiaan Boerkamp, Ziyaad Jaber, and Tareq\n",
      "Jaber. Nlice: Synthetic medical record generation for effective \n",
      "\n",
      "Images:\n",
      "[img_BDI dialog (1)-1 (1)] two texts from the book, the book of the same\n",
      "[img_BDI dialog (1)-1] two texts from the book, the book of the same\n",
      "[img_ablation2] figure 1 of the number of elements in a different set of cells\n",
      "\n",
      "Answer:\n",
      "[img_BDI dialog (1)-1 (1)] two texts from the book, the book of the same\n",
      "[img_BDI dialog (1)-1 (1)] two texts from the book, the book of the same\n",
      "[img_BDI dialog (1)-1 (1)] two texts from the book, the book of the same\n",
      "[img_BDI dialog (1)-1 (1)] two texts from the book, the book of the same\n",
      "[img_BDI d\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Query: Which chart shows an upward trend in patient vitals, and what does the text say about it?\n",
      "\n",
      "EVIDENCE (text ids): ['doc710', 'doc321', 'doc400']\n",
      "EVIDENCE (image ids): ['img_trust game plot (1)', 'img_trust game plot', 'img_unnamed (2)']\n",
      "\n",
      "Result (refused?): False\n",
      "System: You are an expert assistant for clinical research and medical writing. Answer using ONLY the evidence below and include inline citations in the form [doc_id] or [img_id]. If the evidence does not support a confident answer, reply exactly: REFUSE: insufficient reliable evidence.\n",
      "\n",
      "Query: Which chart shows an upward trend in patient vitals, and what does the text say about it?\n",
      "\n",
      "Evidence:\n",
      "Text:\n",
      "[doc710] Three physicians participated in our study: two with two years of Internal Medicine training (Post\n",
      "Graduate Year 2, PGY-2) and one general physician. This composition allowed us to capture a range\n",
      "34\n",
      "[doc321] Three physicians participated in our study: two with two years of Internal Medicine training (Post\n",
      "Graduate Year 2, PGY-2) and one general physician. This composition allowed us to capture a range\n",
      "34\n",
      "[doc400] What does the circle in image D surround? A: Abnormal mitotic figures B: Central keratinization C: Frank atypia D: Areas of necrosis \n",
      "Multidisciplinary Team (MDT)\n",
      "Log Ans\n",
      "Integrated Care Team (ICT)\n",
      "Report\n",
      "Primary Care Clinician (PCC)\n",
      "PromptingAns\n",
      "Collaborative Discussion\n",
      "LowModerateHigh\n",
      "M-turnsN-rounds\n",
      "Medical Query2. Recruitment1. Complexity Check\n",
      "\n",
      "Images:\n",
      "[img_trust game plot (1)] a plot with a plot plot and a plot plot\n",
      "[img_trust game plot] a plot with a plot plot and a plot plot\n",
      "[img_unnamed (2)] the different types of the different types of the different types of the different types of the different types of the\n",
      "\n",
      "Answer:\n",
      "[img_trust game plot] a plot with a plot and a plot\n",
      "[img_unnamed (3)] the different types of the different types of the different types of the different types of the different types of the different types of the different types of the different types of the different types of the different types of the different types of the different types of the different types of the different types of the different types of the different types of the different types of the different types of the different types of the different types of the dif\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Query: Refine the following research claim: 'Multi-agent systems improve clinical note summarization.'\n",
      "\n",
      "EVIDENCE (text ids): ['doc554', 'doc165', 'doc685']\n",
      "EVIDENCE (image ids): ['img_ablation2', 'img_ablation2 (1)', 'img_ablation1']\n",
      "\n",
      "Result (refused?): False\n",
      "System: You are an expert assistant for clinical research and medical writing. Answer using ONLY the evidence below and include inline citations in the form [doc_id] or [img_id]. If the evidence does not support a confident answer, reply exactly: REFUSE: insufficient reliable evidence.\n",
      "\n",
      "Query: Refine the following research claim: 'Multi-agent systems improve clinical note summarization.'\n",
      "\n",
      "Evidence:\n",
      "Text:\n",
      "[doc554] Nidhi Rohatgi, Poonam Hosamani, William Collins, Neera Ahuja, Curtis P. Langlotz, Jason\n",
      "Hom, Sergios Gatidis, John Pauly, and Akshay S. Chaudhari. Adapted large language models can\n",
      "outperform medical experts in clinical text summarization. Nature Medicine, 30(4):1134–1142,\n",
      "February 2024.\n",
      "[80] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, J\n",
      "[doc165] Nidhi Rohatgi, Poonam Hosamani, William Collins, Neera Ahuja, Curtis P. Langlotz, Jason\n",
      "Hom, Sergios Gatidis, John Pauly, and Akshay S. Chaudhari. Adapted large language models can\n",
      "outperform medical experts in clinical text summarization. Nature Medicine, 30(4):1134–1142,\n",
      "February 2024.\n",
      "[80] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, J\n",
      "[doc685] answer.\n",
      "case studies reveals how our framework provides an environment for agents to collaborate, gather\n",
      "information, moderate and make final decisions in complex medical scenarios.\n",
      "33\n",
      "\n",
      "Images:\n",
      "[img_ablation2] figure 1 of the number of elements in a different set of cells\n",
      "[img_ablation2 (1)] figure 1 of the number of elements in a different set of cells\n",
      "[img_ablation1] a bar chart showing the percentage of the average and average of people\n",
      "\n",
      "Answer:\n",
      "[img_ablation2 (1)] figure 1 of the number of elements in a different set of cells\n",
      "[img_ablation2 (1)] figure 1 of the number of elements in a different set of cells\n",
      "[img_ablation2 (1)] figure 1 of the number of elements in a different set of cells\n",
      "[img_ablation2 (1)] figure 1 of the number of elements in a different set of cells\n",
      "[img_ablation2 (1)] figure 1 of the number of elements in a different set of c\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"Summarize the key steps to sanitize PHI from a discharge summary.\",\n",
    "    \"Which chart shows an upward trend in patient vitals, and what does the text say about it?\",\n",
    "    \"Refine the following research claim: 'Multi-agent systems improve clinical note summarization.'\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Query:\", q)\n",
    "    res = generate_rag_answer(q, k_text=3, k_images=3)\n",
    "    print(\"\\nEVIDENCE (text ids):\", [t[0] for t in res[\"evidence\"][\"text\"]])\n",
    "    print(\"EVIDENCE (image ids):\", [i[0] for i in res[\"evidence\"][\"images\"]])\n",
    "    print(\"\\nResult (refused?):\", res[\"refused\"])\n",
    "    print(res[\"text\"][:2000])\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyQ1fFIY3w7J"
   },
   "source": [
    "# 📈 Preview retrievals across a small query set and save a CSV\n",
    "This helps produce `trackB_multimodal_preview.csv` for submission and quick checks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df_preview\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"query\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Summarize PHI sanitization best practices\",\n          \"Find figure showing lab trend\",\n          \"Which section discusses model validation?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text@3\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"images@3\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_text\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 3,\n        \"max\": 3,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_images\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 3,\n        \"max\": 3,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df_preview"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-fc6bc89f-1c0e-4409-b49c-9ca795f13e39\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>text@3</th>\n",
       "      <th>images@3</th>\n",
       "      <th>n_text</th>\n",
       "      <th>n_images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Summarize PHI sanitization best practices</td>\n",
       "      <td>[doc1256, doc1009, doc764]</td>\n",
       "      <td>[img_BDI dialog (1)-1 (1), img_BDI dialog (1)-...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Find figure showing lab trend</td>\n",
       "      <td>[img_ablation1 (1), img_ablation1, doc1467]</td>\n",
       "      <td>[img_unnamed (2), img_unnamed (2) (1), img_abl...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which section discusses model validation?</td>\n",
       "      <td>[doc1012, doc1259, doc1258]</td>\n",
       "      <td>[img_comparison, img_comparison (1), img_ablat...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fc6bc89f-1c0e-4409-b49c-9ca795f13e39')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-fc6bc89f-1c0e-4409-b49c-9ca795f13e39 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-fc6bc89f-1c0e-4409-b49c-9ca795f13e39');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-fcb82a20-34cc-43a5-830b-270113ce9aeb\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fcb82a20-34cc-43a5-830b-270113ce9aeb')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-fcb82a20-34cc-43a5-830b-270113ce9aeb button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "  <div id=\"id_485d2fc1-9bcf-4fbe-9d83-b11bad1e4270\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_preview')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_485d2fc1-9bcf-4fbe-9d83-b11bad1e4270 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('df_preview');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                       query  \\\n",
       "0  Summarize PHI sanitization best practices   \n",
       "1              Find figure showing lab trend   \n",
       "2  Which section discusses model validation?   \n",
       "\n",
       "                                        text@3  \\\n",
       "0                   [doc1256, doc1009, doc764]   \n",
       "1  [img_ablation1 (1), img_ablation1, doc1467]   \n",
       "2                  [doc1012, doc1259, doc1258]   \n",
       "\n",
       "                                            images@3  n_text  n_images  \n",
       "0  [img_BDI dialog (1)-1 (1), img_BDI dialog (1)-...       3         3  \n",
       "1  [img_unnamed (2), img_unnamed (2) (1), img_abl...       3         3  \n",
       "2  [img_comparison, img_comparison (1), img_ablat...       3         3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: trackB_multimodal_preview.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "rows = []\n",
    "preview_queries = [\n",
    "    \"Summarize PHI sanitization best practices\",\n",
    "    \"Find figure showing lab trend\",\n",
    "    \"Which section discusses model validation?\",\n",
    "]\n",
    "for q in preview_queries:\n",
    "    t_hits = retrieve_text_by_text(q, 3)\n",
    "    i_hits = retrieve_images_by_text(q, 3)\n",
    "    rows.append({\n",
    "        \"query\": q,\n",
    "        \"text@3\": [d for d,_,_ in t_hits],\n",
    "        \"images@3\": [i for i,_,_ in i_hits],\n",
    "        \"n_text\": len(t_hits),\n",
    "        \"n_images\": len(i_hits),\n",
    "    })\n",
    "df_preview = pd.DataFrame(rows)\n",
    "display(df_preview)\n",
    "df_preview.to_csv(\"trackB_multimodal_preview.csv\", index=False)\n",
    "print(\"Saved: trackB_multimodal_preview.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCCiZ-LF3_FC"
   },
   "source": [
    "# 💾 Save run config and counts (trackB_run_config.json)\n",
    "Records embedding models and counts for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: trackB_run_config.json\n"
     ]
    }
   ],
   "source": [
    "import json, datetime\n",
    "cfg = {\n",
    "  \"timestamp\": datetime.datetime.now().isoformat(timespec=\"seconds\"),\n",
    "  \"project\": \"Multi-Agent Medical RAG (Track B)\",\n",
    "  \"models\": {\n",
    "    \"text_embedding\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"clip\": \"clip-ViT-B-32\",\n",
    "    \"image_caption\": \"Salesforce/blip-image-captioning-base\",\n",
    "    \"generator\": \"distilgpt2\"\n",
    "  },\n",
    "  \"counts\": {\n",
    "    \"n_text_chunks\": int(len(text_corpus)),\n",
    "    \"n_images\": int(len(df_imgs))\n",
    "  },\n",
    "  \"paths\": {\"images_dir\": str(IMG_DIR), \"corpus_dir\": str(TXT_DIR)}\n",
    "}\n",
    "json.dump(cfg, open(\"trackB_run_config.json\",\"w\"), indent=2)\n",
    "print(\"Saved: trackB_run_config.json\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
